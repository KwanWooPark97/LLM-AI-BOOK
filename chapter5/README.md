# GPU 효율적인 학습  

딥러닝 모델이 입력 데이터를 처리해 결과를 내놓을 때까지 많은 행렬 곱셈 연산을 처리한다. GPU는 이런 계산을 병렬로 여러 개 처리하는데 특화된 장치이다.  
요즘 LLM은 변수가 기본 억을 넘어가니 GPU도 1개가 아닌 여러개가 필요한데 GPU는 비싸다.  
따라서 이 GPU를 어떻게 효율적으로 사용할지 기술 발전이 빠르게 이뤄지고 있다.  
5장에서는 LORA와 QLORA를 살펴본다.  
***  
컴퓨터는 일반적으로 32비트 부동소수점을 사용한다. 쉽게 float32 형식이다. 하지만 파라미터가 몇 억개를 넘어가는 LLM에서는 32비트로 부족한 경우가 있다.  
그래서 용량이 점점 커지고 메모리가 늘어나니 이를 줄이기 위해 성능은 유지하면서 더 적은 비트의 데이터 타입을 사용하는 방향으로 딥러닝 분야가 발전했다. 최근에는 주로 16비트를 사용한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_bit.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_bit.png)  

위 그림에서 s는 부호(sign), e는 지수(exponent), m은 가수(mantissa)를 의미한다.  
지수는 수를 표현할 수 있는 범위의 크기를 결정하고 가수는 표현할 수 있는 수의 촘촘함을 결정한다.  
fp32에서 fp16과 bf16은 비트를 반으로 줄여서 표현할 수 있는 범위나 세밀함이 제한된다.  
하지만 fp16은 지수를 반으로 줄여서 표현할 수 있는 범위가 좁아 딥러닝 연산 과정에서 수를 제대로 표현하지 못하는 문제가 있다.  
따라서 bf16을 사용해 수를 표현할 수 있는 범위는 넓지만 세밀함이 적은 숫자 형태를 사용한다.   
**중요**  
GPU메모리 계산은 간단하다 모델의 파라미터 수에 파라미터 당 비트 수를 곱하면 된다.  
예를들어 1B(10억)개의 파라미터 모델이 있는데 bf16을 사용했다면 16비트는 2바이트이므로 1B*2를 해서 2B이고 이는 GPU 메모리의 2GB를 차지한다는 말이 된다.  
####  
***  
모델 파라미터의 데이터 타입이 더 많은 비트를 사용할수록 모델의 용량이 커지기 때문에 더 적은 비트로 모델을 표현하는 양자화(quantization)기술이 개발됐다.  
fp32로 표현하던 수를 fp16으로 표현하면 당연히 수의 품질이 떨어져서 모델의 성능이 저하된다. 따라서 최대한 정보의 소실없이 유지하는 것이 핵심 과제이다.  
####  
간단한 예로 양자화 방법을 살펴보자.  
만약 데이터들의 분포가 아래와 같다고 생각해보자.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_data.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_data.png)  
먼저 fp32로 표현하던걸 int8로 표현하려고 한다. 그럼 이때 fp32의 최대치와 최소치를 int8의 최대치와 최소치에 대응하면 위의 데이터 분포를 가지고 생각해보면 양쪽 끝부분은 낭비되어버린다.  
그럼 fp32의 최대치, 최소치가 아닌 데이터의 최대치, 최소치를 기준으로 int8을 대응시키면 이러한 문제는 해결될 것이다.  
하지만 만약 데이터에 이상치가 있다면 문제가 된다. 왼쪽 끝이나 오른쪽 끝에 있는 데이터가 정상 데이터가 아니라 이상치라면 이 데이터는 의미없는 데이터이므로 또 다시 낭비가 생겨버린다.  
이러한 문제를 해결하는 방법은 책에서 2가지 설명한다.  
1. 전체 데이터의 최댓값을 구하는게 아니라 3개씩 묶어서 구하고 변환하면 이상치가 전체 데이터가아닌 3개의 데이터에만 영향을 끼친다.
2. 데이터의 크기에 따라 등수를 매겨서 int8에 대응한다. 대신 모든 입력에 등수를 매기고 대응하기 때문에 계산량이 많고 메모리도 사용해야된다.
####  
***  
GPU 메모리를 코드로 알아본다. 이부분은 코드로 보자.  
GPU 메모리 사용량을 알아보는 코드를 확인하고, 그레디언트 누적과 체킹을 통해 메모리 절약하는 방법을 알아본다.  
분산 학습은 코드가 없으므로 여기서 설명한다.  
2개 이상의 GPU를 사용해 학습시키는 방법을 분산 학습(distributed training)이라고 한다.  
분산 학습의 목적은 모델 학습 속도를 높이는 것과 1개의 GPU로 학습이 어려운 모델을 다루는 것이다.  
모델이 작아 하나의 GPU로 올릴 수 있는 경우 여러 GPU에 각각 모델을 올리고 학습 데이터를 병렬로 처리해 학습 속도를 높일 수 있다. 이를 **데이터 병렬화**라고 한다.   
반대로 모델이 너무 큰 경우 **모델 병렬화**를 한다. 똑같은 개념인데 GPU에 모델을 나눠서 올린다.  
크게는 딥러닝 모델의 층 별로 나눠 GPU에 올리는 파이프라인 병렬화와 한 층의 모델도 나눠서 GPU에 올리는 텐서 병럴화가 있다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_gpus.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_gpus.png)   
이렇게 나눠버리면 계산 결과가 달라질까봐 걱정하는데 그에 대한 해답은 아래 그림으로 보여준다.  

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_par.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_par.png)   

분산 학습을 사용하는 경우 모델이 작으면 데이터 병렬화를 통해 학습 속도를 높일 수 있고, 모델이 클 경우 여러 GPU에 하나의 모델을 올려 학습을 가능하게한다.  
데이터 병렬화를 사용하는 경우 동일한 모델을 여러 GPU에 올리기 때문에 중복으로 모델을 저장하면서 메모리 낭비가된다. 이를 줄이는 방법을 알아본다.  
####  
***  
## ZeRO(Zero Redundancy Optimizer)  
하나의 모델을 모델 병렬화 처럼 모델을 나눠 여러 GPU에 올리고 각 GPU에서는 자신의 모델 부분의 연산만 수행하고 그 상태를 저장하면 메모리를 효율적으로 사용하면서 속도도 빠르게 유지할 수 있다는게 컨셉이다.   
https://arxiv.org/abs/1910.02054  

***  
LLM 모델이 커지면서 모든 파라미터를 학습하는 전체 미세 조정을 수행하기 어려워졌다. 따라서 일부 파라미터만 학습하는 PEFT(Parameter Efficient Fine-Tuning) 방법 연구가 활발하다.  
그중에서도 오픈 소스 LLM학습에서 가장 주목받고 많이 활용되는 학습 방법은 모델에 일부 파라미터를 추가하고 그 부분만 학습하는 LORA 방식이다.  
## LORA(Low-Rank Adaptation)

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_LORA.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_LORA.png)   

만약 입력이 d 차원이고 출력이 d차원인 모델이 있다 그러면 모델의 파라미터는 d*d 만큼 있을 것이다.  
만약 d가 매우 크다면 파라미터 수가 엄청나게 증가하고 미세 조정을하든 학습을 하든 모든 파라미터를 이용하므로 메모리 사용량이 크거나 학습 시간이 오래걸린다.  
하지만 위에 그림처럼 파라미터 W를 고정하고 차원이 (d,r)과 (r,d)인 행렬 두개를 곱하여 (d,d)의 행렬을 만들 수 있다.  
r을 d보다 작게하면 기존 보다 훨씬 적은 파라미터 업데이트로 모델을 학습시킬 수 있다.  
물론 행렬 a와 b가 훨씬 작더라도 파라미터가 추가 되는 것이기 때문에 용량 자체는 아주 작게 증가한다.  
하지만 앞에서 GPU메모리에는 모델 파라미터뿐만 아니라 그레디언트,옵티마이저 메모리,순전파 메모리가 있다고 했다.  
LORA는 이 나머지 메모리의 크기를 확실히 줄어버리므로 전체적으로 메모리가 줄어드는 효과가 있다.   
####   
LORA를 사용하기 위해 결정해야 할 사항은 크게 세 가지다.  
1. 위 그림에서 보듯이 r을 몇으로 할지이다. r이 작으면 메모리는 확실히 줄지만 그만큼 모델이 학습할 수 있는 용량이 작아지기 때문에 학습 데이터의 패턴을 충분히 학습하지 못할 수 있다. 따라서 실험을 통해 적절한 r을 찾아야한다.
2. 추가한 파라미터를 기존 파라미터에 얼마나 많이 반영할지 결정하는 알파가 있다. 보통 LORA는 (알파/r)의 가중치를 줘서 학습 시킨다. 알파가 커질수록 새롭게 학습한 파라미터의 중요도가 높아진다. 따라서 실험을 통해 r과함께 알파를 찾는다.
3. 모델에 있는 많은 파라미터 중에서 어떤 파라미터를 재구성할지 결정해야한다. 트랜스포머를 예를 들면 q,k,v와 fnn층이 있는데 이 모든 층을 LORA 적용할지 아니면 일부만 할지 정해야한다. 보통 전부 한다.

이 다음에는 허깅페이스를 이용한 LORA 사용방법을 코드로 알아본다. 예제 5.12  
####  
***  
## QLORA  

양자화의 핵심 과제는 기존 데이터의 정보를 최대한 유지하면서 더 적은 비트를 사용하는 데이터 형식으로 변환하는 것이다.  
하지만 앞에서 언급했듯이 데이터의 순서를 찾고 정렬하면 연산량이 많고 메모리를 사용한다.  
여기서 만약 기존 데이터의 분포를 알고 있다면 많은 연산이나 메모리 사용 없이도 빠르게 데이터의 순위를 정할 수 있다.  
예를 들어, 입력 데이터가 정규 분포를 따른다는 걸 알고 있다면 데이터를 균등하게 분기하기 위해 사용할 경계 값을 쉽게 찾을 수 있다.  
만약 4비트만 사용해 16개의 수를 사용한다면 정규 분포의 면적을 16등분할 수 있는 경계값을 찾고 입력 데이터가 경곗값 기준으로 큰지 작은지에 따라 0~15로 대응할 수 있다.  
위에서 입력 데이터가 정규 분포라고 가정을 했는데 사실 학습된 모델 파라미터는 거의 정규 분포에 가깝다고 연구 결과가 있다.  
따라서 모델의 성능을 거의 유지하면서도 빠른 양자화가 가능해진다. 이를 이용한것이 QLORA이다.  

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_qlora.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/5_qlora.png)   

QLORA는 양자화를 2번한다. 64개의 모델 파라미터를 하나의 블록으로 묶어서 NF4로 양자화 시킨다. 이때 양자화 된 수의 의미는 정규 분포의 표준편차이다.  
그리고 양자화된 상수를 256개 모아서 8비트로 양자화를 한다.  
이렇게 2차 양자화를 하면 이전에는 야자화 상수를 저장하기 위해 32비트 데이터 256개를 저장했지만 이후에는 8비트 데이터 256개와 양자화 상수 C를 저장하기 위한 32비트 데이터 1개만 저장하면 된다.   

####  
이 다음에는 페이지 옵티마이저가 있는데 큰 부분이 아니라 넘어가고  
코드로 QLORA를 불러오는 부분이 있다. 코드를 보자.  
결과를 보면 메모리 사용량이 줄어든 모습을 확인할 수 있다.    
####  
# 정리  
이번 장에서는 GPU 메모리는 효율적으로 사용하는 학습 방법을 알아봤다.  
GPU 메모리에 올라가는 데이터 종류, 데이터 타입, 양자화를 알아봤다.  

## 단일 GPU  
그레이디언트 누적과 그레이디언트 체크포인팅을 공부했다.  
그레이디언트 누적은 제한된 GPU 메모리로도 큰 배치 크기로 학습하는 효과를 얻는다.  
그레이디언트 체크포인팅은 순전파 상태를 일부만 저장해 PGU 메모리 사용량을 줄인다.  

## 여러 GPU=분산 학습  
모델 병렬화와 데이터 병렬화가 있다.  
모델 병렬화는 파이프라인 병렬화와 텐서 병렬화로 나눈다.  
데이터 병렬화는 동일한 모델을 여러 GPU에 올려 비효율적으로 사용하는 것을 중복 데이터를 줄여 사용하는 ZeRO 방법을 공부했다.  
####  
모델이 커지면서 모델 전체를 업데이트 하는 전체 미세 조정이 어려워져서 모델의 일부만 학습하는 peft 방식 연구가 증가했다.  
크게 LoRA와 QLoRA를 공부했다.  
