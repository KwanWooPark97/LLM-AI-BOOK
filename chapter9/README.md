# LLM 어플리케이션 개발하기  

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/9_1.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/9_1.png)   
그림에서 A,B,C는 LLM에 답변에 필요한 정보를 제공하는 기능을 수행한다.  
GPT는 환각 현상이 일어나서 필요한 정보를 프롬프트에 함께 전달하는 RAG를 사용하면 환각 현상을 줄일 수 있었다.  
그림에서 A는 검색하고 싶은 데이터를 데이터 소스에서 가져와 임베딩 모델을 통해 임베딩 벡터로 만들고 이를 DB에 저장하는 과정이다.  
C는 DB에서 요청과 관련된 데이터를 검색하고 검색한 결과를 프롬프트에 반영하는 과정이다.  
B는 검색한 문서를 사용자의 프롬프트에 반영하는 과정이다.  
앞에서 LLM 추론을 효율적으로 하는 다양한 기술을 공부했다. LLM 추론을 줄이기위해 이전에 같거나 비슷한 요청이 있었다면 그 결과를 활용하는 LLM 캐시를 활용한다.  
D가 그런 역할을 하며 비슷한 요청이 있었다면 추론하지않고 그 결과를 가져다 사용한다. 없었다면 E처럼 LLM 추론을 시행한다.  
LLM이 생성한 결과에는 부적절한 내용이 포함되지 않도록 해야한다. 이를 위해 F는 벡터 DB에서 검색한 결과를 확인하고, G는 LLM이 생성한 결과에 문제가 없는지 검증한다.  
서비스에 들어온 사용자의 요청과 LLM의 응답을 기록해야한다. 기록안하면 사용자의 문의에 대응하기 어렵고 서비스가 잘 작동하는지 확인할 수 없다.  
H는 사용자의 요청과 LLM 시스템의 생성 결과를 기록하는 모니터링 과정이다.  

## 검색 증강 생성(RAG)  
RAG란 LLM에게 단순히 질문이나 요청만 전달하고 생성하는 것이 아니라 답변에 필요한 충분한 정보와 맥락을 제공하고 답변하도록 하는 방법을 말한다.  
그림에서는 북쪽에 해당하는 과정이다.  
실은 이미 다른 repo에서 다룬 내용이다보니 대부분 알고있는거라 이렇게 간단히 정리했다.  
LLM 오케스트레이션 도구는 LLM 애플리케이션을 위한 다양한 구성요소를 연결하는 프레임워크로 대표적으로 라마인덱스,랭체인,캐노파 등이 있다.  
이 책에서는 라마인덱스를 사용한다.  

### 데이터 저장  
그림에서 A는 데이터 소스, 임베딩 모델, 벡터 DB로 구성된다.  
데이터 소스는 텍스트, 이미지와 같은 비정형 데이터가 저장된 데이터 저장소를 의미한다.  
데이터 소스의 텍스트를 임베딩 모델을 사용해 임베딩 벡터로 변환한다.  
변환된 임베딩 벡터는 벡터 사이의 거리를 기준으로 검색하는 특수한 DB인 벡터 DB에 저장한다.  
텍스트 임베딩 모델에는 대표적인 상업용 모델로 OpenAI의 text-embedding-ada-002가 있고 오픈소스로는 Sentence-Transformers 라이브러리를 이용한다.  

벡터 DB는 임베딩 벡터의 저장소이고 입력한 벡터와 유사한 벡터를 찾는 기능을 제공한다. 대표적인 DB는 크로마, 밀버스같은 오픈소스와 파인콘, 위비에이트 같은 상업 서비스가 있고  
최근에는 PostgreSQL 같은 관계형 DB에서도 벡터 검색 기능을 도입하고 강화한다.  
처음에는 문서를 임베딩 모델을 통해 임베딩 벡터로 변환하고 DB에 저장한다. 검색을 수행하는 경우 임베딩 모델을 통해 검색 쿼리를 벡터로 변환해 유사도를 계산한다.  

### 프롬프트에 검색 결과 통합  



