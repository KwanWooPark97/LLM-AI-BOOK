# 모델 가볍게 만들기  
서비스를 위해 LLM을 배포하는 경우 많은 비용은 GPU에서 주로 발생한다.  
그래서 GPU를 가능하면 적게 사용해서 비용을 낮추고 효율적인 서빙을 해야한다.  
따라서 이번 장에서는 모델의 성능을 일부 희생하더라도 비용을 크게 낮출 수 있는 방법을 집중적으로 살펴본다.  
####  
LLM의 추론에서는 동일한 연산을 반복적으로 수행하면서 한 토큰씩 생성한다. 이때 발생하는 동일한 연산을 최대한 줄이기 위해 계산 결과를 저장하는 KV 캐시를 사용한다.  
KV 캐시는 중복 계산을 줄여 추론 속도를 높이는 데 도움을 주지만 결과를 저장하기 때문에 GPU 메모리를 많이 사용한다.    
KV 캐시와 모델 파라미터를 저장하는데 많은 GPU 메모리를 사용하면 더 많은 데이터를 처리하지 못하므로 다양한 추론 효율화 방안을 위해   
1. 최적의 배치 크기
2. 멀티 쿼리 어텐션
3. 그룹 쿼리 어텐션  
을 살펴본다.    
그리고 모델을 저장할 때 더 적은 메모리를 사용하도록 또 양자화를 살펴본다.
1. 비츠앤바이츠 라이브러리
2. GPTQ
3. AWQ

마지막으로 더 크고 성능 좋은 선생 모델의 생성 결과를 통해 더 작고 효율적인 학생 모델을 학습하는 지식 증류를 공부한다.  

## 언어 모델이 언어를 생성하는 방법  
앞에서 계속 이야기 했던 개념이라 어느정도 익숙해짐  
언어 모델은 입력한 텍스트의 다음에 올 토큰의 확률을 계산하고 갖아 확률이 높은 토큰을 추가하면서 토큰을 생성해감  
언어 모델이 생성을 마치는 경우는 2가지로   
1. EOS 토큰이 생성될 때
2. 설정한 최대 길이에 도달했을 때

이 경우에 해당하기 전까지 계속 순환하면서 새로운 토큰을 추가한 텍스트를 다시 모델에 입력하는 과정을 반복함  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_1.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_1.png)   

위에 그림처럼 언어 모델은 입력 텍스트를 기반으로 바로 다음 토큰만 예측하는 자기 회귀적(auto regressive)특성을 갖는다.  
하지만 맨 처음 입력해준 프롬프트는 이미 작성된 텍스트이기 때문에 한 번에 하나씩 토큰을 처리할 필요 없이 동시에 병렬적으로 처리 할 수 있다.  
따라서 프롬프트가 길다고 하더라도 다음 토큰 1개를 생성하는 시간과 비슷한 시간이 걸린다.  
이런 이유로 추론 과정을 프롬프트를 처리하는 단계인 사전 계산 단계(prefill phase)와 이후 한 토큰씩 생성하는 디코딩 단계(decoding phase)로 구분한다.  

위의 그림을 참고해서 생각해보면 "검은 고양이가 밥을" 이라는 문장은 언어 모델의 순환이 끝날 때 까지 반복해서 입력으로 들어간다.  
이는 동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산을 반복적으로 수행하기 때문에 비효율적이된다.  
따라서 동일한 입력 토큰에 대해서는 키,값 벡터로의 변환을 수행하지 않고 계산 결과를 저장해서 생성 속도를 높이는 **KV 캐시**를 공부한다.  

## 중복 연산을 줄이는 KV 캐시  
KV 캐시는 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해   
먼저 계산했던 키와 값 계산 결과를 저장하기 때문에 Key-Value 캐시라고 이름이 붙었다.    
KV 캐시는 계산 결과를 저장하고 있기 때문에 GPU 메모리를 사용한다.  
따라서 GPU 메모리에는 크게 모델 파라미터와 KV 캐시 두 부분으로 나누어진다.  
KV 캐시는 그럼 메모리 계산을 어떻게 하냐?   
메모리=2 바이트 * 2(K와 V)* 레이어 수 * 토큰 임베딩 차원 * 최대 시퀀스 길이 * 배치 크기  

위 식을 이용해 메모리를 계산하면 파라미터가 13B인 라마 모델의 경우 배치 크기가 5일 때 NVIDIA의 비싼 GPU인 A100 40GB 의 메모리를 거의 다 차지해버린다.  
그럼 이 배치 크기를 어느 정도로 사용해야 효율적으로 사용했다고 표현할 수 있을까??  
####  
## GPU 구조와 최적의 배치 크기  
보통 서빙이 효율적인지 판단하는 큰 기준은  
1. 비용
2. 처리량: 시간당 처리한 요청 수 (query/s)
3. 지연 시간: 하나의 토큰을 생성하는데 걸리는 시간(token/s)

따라서 더 적은 비용으로 더 많은 요청을 처리하면서 생성한 다음 토큰을 빠르게 전달할 수 있다면 효율적인 서빙이라고 한다.  
만약 비용이 GPU의 종류와 수에만 영향을 받는다고 하면, 같은 GPU로 처리량을 높이고 지연 시간을 낮춰야 효율적인 서빙이 된다.  
먼저 GPU의 구조를 살펴보면 아래 그림과 같다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_2.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_2.png)   

하나의 GPU에는 여러개의 스트리밍 멀티프로세서(Streaming Multiprocessors, SM)으로 구성되어있고  
각 SM에는 연산을 수행하는 부분과 계산한 값을 저장하는 SRAM이 있다. (SRAM은 보통 L1 캐시 또는 공유 메모리 라고도 한다.)  
SRAM들은 값을 저장은 하지만 큰 메모리를 가지고 있지 않으므로 큰 고대역폭 메모리(High Bandwidth Memory,HBM)에 큰 데이터를 저장한다.  

모델의 추론 과정에서 연산을 수행하는 데만 시간이 걸리는 것이 아니고 앞에서 말했듯 GPU는 HBM에 데이터를 저장해두고 사용하기 때문에 이 데이터를 SRAM으로 이동시키는데 필요한 시간도 있다.  
따라서 일반적으로 배치 크기가 커지면 연산에 필요한 시간이 증가하지만 모델 파라미터 이동 시간은 변함 없으므로  
**두 과정이 같은 시간이 걸릴 때**가 최적의 배치크기라고 생각한다. 만약 두 시간이 다르다면 어느 한쪽 과정은 반드시 멈추는 시간이 필요하므로 비효율이 발생한다.  
####  
만약 최적의 배치 크기를 B*라고 가정할때  
배치 크기가 작으면 모델 파라미터를 이동시키느라 연산이 멈추는 비효율이 생긴다. 이를 **메모리 바운드(memory bound)**라고 한다.  
반대로 배치크기가 커지면 연산이 오랜 시간이 걸려 지연 시간이 길어진다. 이를 **연산 바운드(compute bound)**라고 한다.  
위에서 말한 A100 GPU를 통해 계산하면 최적의 배치 크기는 100이 나온다.  
하지만 배치 크기가 5일때 OOM이기 때문에 최대 배치 크기가 최적의 배치 크기에 가까워질 수 있는 방법을 찾아야한다.  
위에서 언급했듯이 GPU 메모리는 크게 모델 파라미터와 KV 캐시 부분으로 나눠지므로 배치 크기를 키우는 방안은 모델의 용량을 줄이는 방법과 KV 캐시의 용량을 줄이는 방법으로 나뉜다.  
먼저 KV 캐시의 용량을 줄이는 방법인 멀티 쿼리 어텐션과 그룹 쿼리 어텐션을 살펴보고 뒤에 모델 용량을 줄이는 양자화와 지식 증류를 살펴본다.  

## KV 캐시 메모리 줄이기  
트랜스포머는 일반적으로 멀티 헤드 어텐션을 사용해서 쿼리와 키 사이에 다양한 측면에 관련성을 반영하여 성능을 높였다.  
하지만 KV 캐시를 사용하면 멀티 헤드 수 만큼 KV가 곱해지기 때문에 많은 메모리를 사용하고 많은 데이터를 불러오므로 속도가 느려진다.   
이를위해 처음에는 모든 쿼리 벡터가 하나의 키와 값 벡터를 공유하는 멀티 쿼리 어텐션이 개발되었다.  
말 그대로 키와 값 벡터가 한개이므로 성능은 떨어지지만 속도는 확실히 빨라진 모습을 보여준다.  
2023년에 구글에서 그룹 쿼리 어텐션을 개발해 아래 그림과 같은 형태의 키와 쿼리를 이용해 계산하도록 만들었다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_3.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_3.png)   

멀티 쿼리 어텐션이나 그룹 쿼리 어텐션과 같이 키와 값의 수를 줄이면서 크게 추론 속도 향상과 KV 캐시 메모리 감소의 이득을 볼 수 있다.  
이 결과를 아래 그림과 같이 논문에서 표현했다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_4.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_4.png)   

왼쪽은 Y축 성능 X축 샘플당 시간을 의미하며 보면 알수있다.  
오른쪽은 성능 저하가 뚜렷하기 때문에 키와 값을 줄인 이후에 기존의 학습 데이터로 추가 학습을 수행했을 때 결과이다.  
Y축은 성능을 나타내고 X축은 추가 학습 비율이다.  
그룹 쿼리 어텐션은 처음부터 성능이 큰 차이가 없었지만 학습 데이터의 5%만 추가 학습 해준다면 성능이 거의 동일해진다.  
**따라서 그룹 쿼리 어텐션을 사용하면 키와 값 벡터를 줄여도 성능 하락이 거의 없고 추론 속도도 빨라지고 KV 메모리 사용량 또한 줄어들어 배치 크기를 늘릴 수 있다.**    

####  
## 양자화로 모델 용량 줄이기  
양자화란 부동소수점 데이터를 더 적은 메모리를 사용하는 정수 형식으로 변환해 GPU를 효율적으로 사용하는 방법을 말한다.  
이전에는 32비트 부동소수점 데이터 형식으로 모델 파라미터를 저장했지만 요즘은 16비트로 저장하는 경우가 많아졌다.  
16비트로 해도 7B 모델을 올리는데만 14GB 메모리가 필요하다. 따라서 계속해서 모델의 용량을 줄이려는 연구는 진행중이다.  
16비트 파라미터는 보통 8,4,3 비트로 양자화 하는데 최근에는 4비트로 모델 파라미터를 양자화 하고 계산은 16비트로 하는 W4A16(Weight 4bits and Activation 16bits)을 주로 활용한다.  
양자화는 수행하는 시점에 따라 두가지로 나눈다.  
1. 학습 후 양자화(Post-Training Quantization, PTQ)
2. 양자화 학습(Quantization-Aware Training, QAT)

LLM은 보통 학습에 많은 자원이 들기 때문에 새로운 학습이 필요한 양자화 학습보다는 학습 후 양자화를 주로 활용한다. (학습을 하면서 양자화도 하면 안 그래도 오래걸리는 학습을 더 걸리게 만들어 버려서 그런듯?)  
2024년 5월 기준 허깅페이서에서 가장 많이 사용하는 양자화는 세 가지이다.  
1. 비츠앤바이츠(bits-and-bytes)
2. GPTQ(GPT Quantization)
3. AWQ(Activation-aware Weight Quantization)

세 가지 방법 모두 각각 소개한다.  

####  
## 비츠앤 바이츠  
비츠앤바이츠는 양자화 방식을 쉽게 사용할 수 있도록 제공하는 양자화 라이브러리다.  
비츠앤바이츠는 크게 두 가지 양자화 방식을 제공한다.  
1. 8비트 연산을 수행하면서도 성능 저하가 거의 없이 성능을 유지하는 8비트 행렬 연산
2. 4비트 정규 분포 양자화 방식

4비트 양자화 방식은 5장에서 QLORA를 설명할때 있떤 그 방식이다. 따라서 책은 8비트 연산만 소개한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_5.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_5.png)   

양자화를 진행하면 어쩔 수 없이 성능이 떨어진다. 그래서 최대한 이를 방지하기 위해 비츠앤바이츠는 위 그림과 같은 방식의 양자화를 수행한다.  
입력 x의 값 중 크기가 큰 이상치가 포함된 열은 별도로 분리하여 16비트 그대로 계산한다.  
입력에서 값이 큰 경우는 중요한 정보를 가지고 있다고 판단해서 정보가 소실되지 않도록 그대로 연산한것이다.  
이상치 벡터끼리 행렬 곱셈을 수행해 16비트 결과를 산출한다.  
나머지는 절대 최댓값을 이용해 양자화를 수행하고 양자화 상수를 구하고 행렬 곱셈을 통해 값을 계산한다.  
두 값을 더하여 최종 16비트 값을 산출하여 사용한다.  

## GPTQ  
GPTQ는 양자화 이전의 모델에 입력 X를 넣었을 때와 양자화 이후의 모델에 입력 X를 넣었을 때 오차가 작아지도록 양자화를 수행한다.  
가중치 행렬을 하나의 블록(박스)라고 했을 때 한 열씩(세로) 양자화를 수행하는데 해당 열보다 왼쪽에 있는 부분은 이미 양자화가 진행된 부분이므로 냅두고  
오른쪽에 있는 부분을 업데이트하며 양자화를 진행한다.  

## AWQ
해당 방법의 기반이 되는 의문은 모든 파라미터가 가지고 있는 가중치가 정말 정보를 고르게 가지고 있을까? 라는 것에서 출발한다.  
AWQ는 모든 파라미터가 동등하게 중요하지는 않으며 특별히 중요한 파라미터의 정보를 유지하면 양자화를 수행하면서도 성능 저하를 막을 수 있다는 아이디어에서 출발했다.  
중요한 파라미터를 어떻게 구분할까?? 간단하게 값이 크다면 중요하다고 예상한다.  
입력 데이터의 활성화 값이 큰 채널의 파라미터가 중요하다고 가정하고 MIT에서 실험을 진행했다.  
상위 1%에 해당하는 모델 파라미터를 찾고 해당 파라미터들만 FP16 타입으로 놔두고 나머지는 양자화를 진행하니 성능 저하가 거의 발생하지 않았다.  
하지만 이렇게 데이터 타입이 섞여 있는 경우 한번에 일괄적으로 연산하기 어려워서 연산이 느려지고 하드웨어 효율성이 떨어진다.  
해결하기 전에 양자화로인해 성능 저하가 생기는 이유를 생각해본다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_6.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_6.png)   
색이 다른 가중치가 중요한 파라미터가 있는 행이라고 가정한다.  
기존 모델 파라미터의 절대 최댓값은 4이고 4비트 정수는 -8~7까지 표현 가능하므로 2배하고 반올림하여 양자화를 실행하면 오른쪽과 같은 결과가 나온다.  
양자화를 실행전에는 서로 다른 값이었는데 양자화를 수행하니 같은 값으로 표현된다. 이는 뭔가 중요한 정보를 잃어버렸다고 할 수 있다.  
그래서 아래와 같이 일정 스케일러를 곱해서 값을 올려준다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_7.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_7.png)   
최종 결과를 보면 이전과 다르게 값이 달라서 정보를 좀 더 지킬 수 있었다는 의미같다.  
그럼 이 스케일러를 몇으로 해야할까?? MIT는 2일 때까지는 성능이 향상 되었지만 너무 커지면 성능이 더 줄어들었다고 한다.  
따라서 적당히 또 실험적으로 찾아야한다.  
####  
***  
## 지식 증류 활용하기  
지식 증류란 모델의 크기가 더 크고 성능이 높은 선생 모델의 생성 결과를 활용해 더 작고 성능이 낮은 학생 모델을 만드는 방법을 말한다.  
학생 모델은 선생 모델의 생성 결과를 모방하는 방식으로 학습하는데, 선생 모델에 쌓은 지식을 더 작은 모델로 압축해 전달한다는 의미로 '증류'라고 표현한다.  
GPT 이전에도 이러한 방식을 활용했지만 최근 언어 모델이 뛰어난 언어 능력을 보이면서 과거에는 학습 데이터셋에 대한 선생 모델의 결과만을 사용했다면  
이제는 선생 모델이 직접 새로운 학습 데이터셋을 구축하고 사람의 판단이 필요한 부분을 선생 모델이 수행하는 등 더 폭넓게 활용되고 있다.  
실제로 앞에서 GPT 개발 과정을 공부했을 때 지시 데이터셋과 선호 데이터셋을 레이블러를 고용해 직접 만들었다면 최근에는 그것들을 모두 LLM에게 시켜서 SOTA 성능을 내고있다.  


# 정리   
이 장에서는 모델의 추론 효율화를 위해 모델의 용량을 줄이는 다양한 방법을 살펴봤다.  
언어 모델의 추론 방식을 살펴보는 것으로 시작했고 중복 연산이 많이 발생하므로 연산 속도를 높이기 위해 KV 캐시를 사용해 어텐션 연산의 키와 값을 저장해 사용했다.  
하지만 GPU 메모리를 차지하면서 배치 크기가 제한되는 문제가 발생했다.  
멀티 쿼리 어텐션과 그룹 쿼리 어텐션은 기존 멀티 헤드 어텐션과 달리 쿼리를 그룹지어 키와 값 벡터를 매칭해서 KV 캐시의 메모리를 줄였다.  
다음으로 양자화를 통해 성능을 최대한 유지하며 메모리를 줄이는 방시을 알아봤다.  
지식 증류는 결과적으로 사용하는 모델의 용량을 낮춰 추론 효율화를 이룬다.
