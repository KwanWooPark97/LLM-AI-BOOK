# 모델 가볍게 만들기  
서비스를 위해 LLM을 배포하는 경우 많은 비용은 GPU에서 주로 발생한다.  
그래서 GPU를 가능하면 적게 사용해서 비용을 낮추고 효율적인 서빙을 해야한다.  
따라서 이번 장에서는 모델의 성능을 일부 희생하더라도 비용을 크게 낮출 수 있는 방법을 집중적으로 살펴본다.  
####  
LLM의 추론에서는 동일한 연산을 반복적으로 수행하면서 한 토큰씩 생성한다. 이때 발생하는 동일한 연산을 최대한 줄이기 위해 계산 결과를 저장하는 KV 캐시를 사용한다.  
KV 캐시는 중복 계산을 줄여 추론 속도를 높이는 데 도움을 주지만 결과를 저장하기 때문에 GPU 메모리를 많이 사용한다.    
KV 캐시와 모델 파라미터를 저장하는데 많은 GPU 메모리를 사용하면 더 많은 데이터를 처리하지 못하므로 다양한 추론 효율화 방안을 위해   
1. 최적의 배치 크기
2. 멀티 쿼리 어텐션
3. 그룹 쿼리 어텐션  
을 살펴본다.    
그리고 모델을 저장할 때 더 적은 메모리를 사용하도록 또 양자화를 살펴본다.
1. 비츠앤바이츠 라이브러리
2. GPTQ
3. AWQ

마지막으로 더 크고 성능 좋은 선생 모델의 생성 결과를 통해 더 작고 효율적인 학생 모델을 학습하는 지식 증류를 공부한다.  

## 언어 모델이 언어를 생성하는 방법  
앞에서 계속 이야기 했던 개념이라 어느정도 익숙해짐  
언어 모델은 입력한 텍스트의 다음에 올 토큰의 확률을 계산하고 갖아 확률이 높은 토큰을 추가하면서 토큰을 생성해감  
언어 모델이 생성을 마치는 경우는 2가지로   
1. EOS 토큰이 생성될 때
2. 설정한 최대 길이에 도달했을 때

이 경우에 해당하기 전까지 계속 순환하면서 새로운 토큰을 추가한 텍스트를 다시 모델에 입력하는 과정을 반복함  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_1.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_1.png)   

위에 그림처럼 언어 모델은 입력 텍스트를 기반으로 바로 다음 토큰만 예측하는 자기 회귀적(auto regressive)특성을 갖는다.  
하지만 맨 처음 입력해준 프롬프트는 이미 작성된 텍스트이기 때문에 하 번에 하나씩 토큰을 처리할 필요 없이 동시에 병렬적으로 처리 할 수 있다.  
따라서 프롬프트가 길다고 하더라도 다음 토큰 1개를 생성하는 시간과 비슷한 시간이 걸린다.  
이런 이유로 추론 과정을 프롬프트를 처리하는 단계인 사전 계산 단계(prefill phase)와 이후 한 토큰씩 생성하는 디코딩 단계(decoding phase)로 구분한다.  

위의 그림을 참고해서 생각해보면 "검은 고양이가 밥을" 이라는 문장은 언어 모델의 순환이 끝날 때 까지 반복해서 입력으로 들어간다.  
이는 동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산을 반복적으로 수행하기 때문에 비효율적이된다.  
따라서 동일한 입력 토큰에 대해서는 키,값 벡터로의 변환을 수행하지 않고 계산 결과를 저장해서 생성 속도를 높이는 **KV 캐시**를 공부한다.  

## 중복 연산을 줄이는 KV 캐시  
KV 캐시는 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해   
먼저 계산했던 키와 값 계산 결과를 저장하기 때문에 Key-Value 캐시라고 이름이 붙었다.    
KV 캐시는 계산 결과를 저장하고 있기 때문에 GPU 메모리를 사용한다.  
따라서 GPU 메모리에는 크게 모델 파라미터와 KV 캐시 두 부분으로 나누어진다.  
KV 캐시는 그럼 메모리 계산을 어떻게 하냐?   
메모리=2 바이트 * 2(K와 V)* 레이어 수 * 토큰 임베딩 차원 * 최대 시퀀스 길이 * 배치 크기  

위 식을 이용해 메모리를 계산하면 파라미터가 13B인 라마 모델의 경우 배치 크기가 5일 때 NVIDIA의 비싼 GPU인 A100 40GB 의 메모리를 거의 다 차지해버린다.  
그럼 이 배치 크기를 어느 정도로 사용해야 효율적으로 사용했다고 표현할 수 있을까??  
####  
## GPU 구조와 최적의 배치 크기  
보통 서빙이 효율적인지 판단하는 큰 기준은  
1. 비용
2. 처리량: 시간당 처리한 요청 수 (query/s)
3. 지연 시간: 하나의 토큰을 생성하는데 걸리는 시간(token/s)

따라서 더 적은 비용으로 더 많은 요청을 처리하면서 생성한 다음 토큰을 빠르게 전달할 수 있다면 효율적인 서빙이라고 한다.  
만약 비용이 GPU의 종류와 수에만 영향을 받는다고 하면, 같은 GPU로 처리량을 높이고 지연 시간을 낮춰야 효율적인 서빙이 된다.  
먼저 GPU의 구조를 살펴보면 아래 그림과 같다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_2.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_2.png)   

하나의 GPU에는 여러개의 스트리밍 멀티프로세서(Streaming Multiprocessors, SM)으로 구성되어있고  
각 SM에는 연산을 수행하는 부분과 계산한 값을 저장하는 SRAM이 있다. (SRAM은 보통 L1 캐시 또는 공유 메모리 라고도 한다.)  
SRAM들은 값을 저장은 하지만 큰 메모리를 가지고 있지 않으므로 큰 고대역폭 메모리(High Bandwidth Memory,HBM)에 큰 데이터를 저장한다.  

모델의 추론 과정에서 연산을 수행하는 데만 시간이 걸리는 것이 아니고 앞에서 말했듯 GPU는 HBM에 데이터를 저장해두고 사용하기 때문에 이 데이터를 SRAM으로 이동시키는데 필요한 시간도 있다.  
따라서 일반적으로 배치 크기가 커지면 연산에 필요한 시간이 증가하지만 모델 파라미터 이동 시간은 변함 없으므로  
**두 과정이 같은 시간이 걸릴 때**가 최적의 배치크기라고 생각한다. 만약 두 시간이 다르다면 어느 한쪽 과정은 반드시 멈추는 시간이 필요하므로 비효율이 발생한다.  
####  
만약 최적의 배치 크기를 B*라고 가정할때  
배치 크기가 작으면 모델 파라미터를 이동시키느라 연산이 멈추는 비효율이 생긴다. 이를 **메모리 바운드(memory bound)**라고 한다.  
반대로 배치크기가 커지면 연산이 오랜 시간이 걸려 지연 시간이 길어진다. 이를 **연산 바운드(compute bound)**라고 한다.  
위에서 말한 A100 GPU를 통해 계산하면 최적의 배치 크기는 100이 나온다.  
하지만 배치 크기가 5일때 OOM이기 때문에 최대 배치 크기가 최적의 배치 크기에 가까워질 수 있는 방법을 찾아야한다.  
위에서 언급했듯이 GPU 메모리는 크게 모델 파라미터와 KV 캐시 부분으로 나눠지므로 배치 크기를 키우는 방안은 모델의 용량을 줄이는 방법과 KV 캐시의 용량을 줄이는 방법으로 나뉜다.  
먼저 KV 캐시의 용량을 줄이는 방법인 멀티 쿼리 어텐션과 그룹 쿼리 어텐션을 살펴보고 뒤에 모델 용량을 줄이는 양자화와 지식 증류를 살펴본다.  

## KV 캐시 메모리 줄이기  
트랜스포머는 일반적으로 멀티 헤드 어텐션을 사용해서 쿼리와 키 사이에 다양한 측면에 관련성을 반영하여 성능을 높였다.  
하지만 KV 캐시를 사용하면 멀티 헤드 수 만큼 KV가 곱해지기 때문에 많은 메모리를 사용하고 많은 데이터를 불러오므로 속도가 느려진다.   
이를위해 처음에는 모든 쿼리 벡터가 하나의 키와 값 벡터를 공유하는 멀티 쿼리 어텐션이 개발되었다.  
말 그대로 키와 값 벡터가 한개이므로 성능은 떨어지지만 속도는 확실히 빨라진 모습을 보여준다.  
2023년에 구글에서 그룹 쿼리 어텐션을 개발해 아래 그림과 같은 형태의 키와 쿼리를 이용해 계산하도록 만들었다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_3.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_3.png)   

멀티 쿼리 어텐션이나 그룹 쿼리 어텐션과 같이 키와 값의 수를 줄이면서 크게 추론 속도 향상과 KV 캐시 메모리 감소의 이득을 볼 수 있다.  
이 결과를 아래 그림과 같이 논문에서 표현했다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_4.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/7_4.png)   

왼쪽은 Y축 성능 X축 샘플당 시간을 의미하며 보면 알수있다.  
오른쪽은 성능 저하가 뚜렷하기 때문에 키와 값을 줄인 이후에 기존의 학습 데이터로 추가 학습을 수행했을 때 결과이다.  
Y축은 성능을 나타내고 X축은 추가 학습 비율이다.  
그룹 쿼리 어텐션은 처음부터 성능이 큰 차이가 없었지만 학습 데이터의 5%만 추가 학습 해준다면 성능이 거의 동일해진다.  
**따라서 그룹 쿼리 어텐션을 사용하면 키와 값 벡터를 줄여도 성능 하락이 거의 없고 추론 속도도 빨라지고 KV 메모리 사용량 또한 줄어들어 배치 크기를 늘릴 수 있다.**    

####  
## 양자화로 모델 용량 줄이기  
양자화란 부동소수점 데이터를 더 적은 메모리를 사용하는 정수 형식으로 변환해 GPU를 효율적으로 사용하는 방법을 말한다.  
이전에는 32비트 부동소수점 데이터 형식으로 모델 파라미터를 저장했지만 요즘은 16비트로 저장하는 경우가 많아졌다.  
16비트로 해도 7B 모델을 올리는데만 14GB 메모리가 필요하다. 따라서 계속해서 모델의 용량을 줄이려는 연구는 진행중이다.  
16비트 파라미터는 보통 8,4,3 비트로 양자화 하는데 최근에는 4비트로 모델 파라미터를 양자화 하고 계산은 16비트로 하는 W4A16(Weight 4bits and Activation 16bits)을 주로 활용한다.  
양자화는 수행하는 시점에 따라 두가지로 나눈다.  
1. 학습 후 양자화(Post-Training Quantization, PTQ)
2. 양자화 학습(Quantization-Aware Training, QAT)

LLM은 보통 학습에 많은 자원이 들기 때문에 새로운 학습이 필요한 양자화 학습보다는 학습 후 양자화를 주로 활용한다. (학습을 하면서 양자화도 하면 안 그래도 오래걸리는 학습을 더 걸리게 만들어 버려서 그런듯?)  
2024년 5월 기준 허깅페이서에서 가장 많이 사용하는 양자화는 세 가지이다.  
1. 비츠앤바이츠(bits-and-bytes)
2. GPTQ(GPT Quantization)
3. AWQ(Activation-aware Weight Quantization)

세 가지 방법 모두 각각 소개한다.  

####  
## 비츠앤 바이츠  
비츠앤바이츠는 양자화 방식을 쉽게 사용할 수 있도록 제공하는 양자화 라이브러리다.  
비츠앤바이츠는 크게 두 가지 양자화 방식을 제공한다.  
1. 8비트 연산을 수행하면서도 성능 저하가 거의 없이 성능을 유지하는 8비트 행렬 연산
2. 4비트 정규 분포 양자화 방식

4비트 양자화 방식은 5장에서 QLORA를 설명할때 있떤 그 방식이다. 따라서 책은 8비트 연산만 소개한다.  


