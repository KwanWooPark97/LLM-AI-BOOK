{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMEN6Pdhq-3v"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.40.1 accelerate==0.30.0 bitsandbytes==0.43.1 auto-gptq==0.7.1 autoawq==0.2.5 optimum==1.19.1 -qqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etO0bk4KrMl8"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "import auto_gptq\n",
        "import awq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oDjUbbcZqIw"
      },
      "source": [
        "## 예제 7.1. 비츠앤바이츠 양자화 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oruzQDHHrDCq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# 8비트 양자화 모델 불러오기\n",
        "bnb_config_8bit = BitsAndBytesConfig(load_in_8bit=True)#8비트 양자화를 수행한다 True\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=bnb_config_8bit)\n",
        "\n",
        "# 4비트 양자화 모델 불러오기\n",
        "bnb_config_4bit = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                     bnb_4bit_quant_type=\"nf4\")# 모델 파라미터 분포가 정규 분포를 가정하고 양자화 하는 방법 5장에서 배웠음\n",
        "\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
        "                                                  low_cpu_mem_usage=True,\n",
        "                                                  quantization_config=bnb_config_4bit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg6WHI8hZqIw"
      },
      "source": [
        "## 예제 7.2. GPTQ 양자화 수행 코드\n",
        "\n",
        "코드 출처: https://huggingface.co/blog/gptq-integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmy7fV8krIJD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
        "\n",
        "model_id = \"facebook/opt-125m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "quantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)# dataset은 양자화에 사용할 데이터 셋을 의미함\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwVIOhpbZqIx"
      },
      "source": [
        "## 예제 7.3. GPTQ 양자화된 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTDHvm55rJb-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/zephyr-7B-beta-GPTQ\",#양자화가 굉장히 오래걸려서 차라리 양자화한 모델을 가져오는게 더 좋다고 생각함\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldjWYu7UZqIx"
      },
      "source": [
        "## 예제 7.4. AWQ 양자화 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyomgkqLrLYi"
      },
      "outputs": [],
      "source": [
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-AWQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
        "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True, trust_remote_code=False, safetensors=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}