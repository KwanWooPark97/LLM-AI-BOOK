# LLM의 기본 상식을 쌓자  
LLM은 다음 단어를 예측하는 방식으로 대량의 텍스트를 학습해서 뛰어난 텍스트 생성 능력을 보여줬다.  
하지만 초기 GPT-3는 이 방식으로 학습했지만 사용자의 요청에 적절히 응답하기보다는 사용자의 말에 이어질 법한 텍스트를 생성했다는 한계가있었다.  
##### 
그럼 어떻게 챗 GPT를 만들었을까?
1. 네이버 지식인과 같은 요청과 답변 형식으로 된 지시 데이터셋(instruction dataset) 을 통해 GPT-3가 사용자의 요청에 응답할 수 있도록 학습시켰다.
2. 사용자가 더 좋아하고 사용자에게 더 도움이 되는 답변을 생성할 수 있도록 추가학습을 시켰다.(여기서 RLHF랑 DPO같은 기술들이 나왔겠지?)
####
****
위에 2번 방식에서 사람들이 더 선호하는 답변을 생성할 수 있도록 모델을 조정하는 방법은 크게 강화학습을 사용하는 방법과 사용하지 않는 방법으로 나눌수있다.  
맨 처음 챗GPT는 PPO를 이용해서 학습시켰지만 PPO의 고질적인 문제인 하이퍼파라미터에 예민하고 이에 따른 학습이 불안정해서 문제가 많이 있었다.  
그래서 안쓰는 기술들이 개발되고 있는데 대표적인게 DPO이다.  
(공부할때 이책을 선택한 이유가 RLHF와 DPO를 설명해준다는 점이 매우 호감이였음)  
####
****
이미 알고있지만 LLM은 보통 인터넷상에 있는 다양한 텍스트 데이터를 수집한 대용량의 텍스트로 사전 학습한다. 2023년 메타에서 공개한 라마-2 모델은 약 10TB 분량의 텍스트를 사전 학습에 사용했는데  
사전 학습 데이터의 경우 코드, 블로그, 기사, 광고 등 다양한 글이 섞여있기 때문에 사전 학습 데이터에서 다음 단어를 예측하는 방법으로 학습하는 경우 LLM이 특정한 형태로 응답하거나 사용자의 요청에 따라 응답하길 기대하기는 어렵다.  
사전 학습 동안은 LLM이 언어에 대한 전체적인 이해도가 높아지고 바로 다음에 올 단어를 점점 더 잘 예측하게된다.  
(데이터가 다양한 형태이고 사용자의 뭔가가 들어가는 경우가 아니라 진짜 다음 단어만 예측하게 학습만 한다는 뜻같음 나중에 사용자 요청에 대한 학습을 따로 진행)  

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_train.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_train.png)
####  
****
LLM도 사용자의 요청에 적절히 응답하기 위해서는 사람이 코딩 테스트를 준비할 때와 비슷하게 요청의 형식을 적절히 해석하고 응답의 형태를 적절히 작성하며 요청과 응답이 잘 연결되도록 추가로 학습한다.  
이를 지도 미세 조정(supervised fine-tuning)이라고 한다.  
학습 데이터에 정답이 있으므로 "지도"가 붙었다.  
지도 미세 조정을 통해 LLM은 사용자의 요청에 맞춰 응답하도록 학습하는데, 이를 정렬(alignment)라고 한다. 사람의 요청과 LLM의 응답이 정렬되도록 한다는 의미이다.  
지도 미세 조정에 사용하는 학습 데이터를 지시 테이터셋(instruction dataset)이라고 부른다. 지시 데이터셋에 비해 사전 학습 데이터셋은 형식이 너무 다양하고, 사용자의 요청에 응답하는 형식의 데이터는 적다.  
이런 문제를 보완하기 위해 사용자의 요구사항과 그에 대한 응답을 구조화한 데이터를 구축하고 언어 모델의 학습에 활용한다.  
위에서 말했듯 지시 데이터셋은 사용자의 요청을 형식에 맞춰 작성하고, 그에 대해 적절한 형식의 응답을 하는 형태다. OpenAI는 이걸 만들기 위해 레이블러를 고용해 13000개가 넘는 지시 데이터셋을 구축해서 학습했다.   
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_ins.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_ins.png)  
위 사진에서 응답을 레이블러가 직접 작성한다.
지시 데이터셋은 실제로 어떤 형태일까?  
https://huggingface.co/datasets/tatsu-lab/alpaca  
위 링크로 들어가면 알파카에 사용한 데이터셋을 볼 수 있다.   
####  
****  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_data.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_data.png)  
지시사항은 사용자의 요구사항을 표현한 문장이다. 입력에는 답변을 하는 데 필요한 데이터가 들어간다. 출력은 지시사항과 입력을 바탕으로 한 정답 응답이다. 텍스트는 지시사항, 입력, 출력을 정해진 포맷으로 하나로 묶은 데이터이다.  
데이터를 보면 지시사항, 입력 ,출력이 있는데 이것을 지시 데이터셋으로 구조화 한 것이 text 부분이다.  
첫 두 줄에서 "작업을 설명한 지시사항과 맥락 정보인 입력을 바탕으로 요청에 대한 적절한 응답을 작성하라"라는 안내를 하고 앞의 것들을 전부 넣어준다.  
이때 LLM이 데이터의 형식을 인식할 수 있도록 ###으로 텍스트를 구분해준다.  
데이터 셋이 어떻게 생겼는지는 확인했다. 그럼 어떻게 학습을 진행할까?  
그냥 사전 학습 때와 동일하게 다음 단어를 예측하는 인과적 언어 모델링(causal language modeling)을 사용해 학습한다. 즉, 지도 미세 조정이라는 이름은 LLM이 학습하는 방식이 달라서가 아니라 학습 데이터셋에 차이가 있어서 지어진것이다.  
####
****
## 좋은 지시 데이터셋이 갖춰야 할 조건  
1. 지시 데이터셋이 얼마나 필요할까?  
메타는 정렬을 위해서는 적은 것이 더 낫다라고 판단했다. 실제로 파라미터가 650억 개인 라마 모델을 정렬 하는데 사용한 지시 데이터셋은 1000개 정도였다.
그뿐 아니라 약 300개의 데이터로 사람이 평가하는 테스트를 했는데 52000개의 데이터셋으로 정렬한 알파카보다 1000개로 정렬한 리마가 응답 품질이 뛰어났고,
구글의 바드, GPT-4와 비교해도 4~50% 답변은 리마가 더 뛰어나거나 비슷한 수준이였다.
####  
2. 지시 데이터셋에서 지시사항이 다양한 형태로 되어 있고 응답 데이터의 품질이 높을수록 정렬한 모델의 답변 품질이 높아진다.  
메타는 지시 데이터셋을 크게 위키하우와 스택익스체인지 데이터를 사용했다. 위키 하우는 질문에 대해 하는 방법(How to) 형식이고 스택익스체인지는 다양하게 있었고 이 중 좋은 답변만 선별했다.
이렇게 필터를 거쳐 질문의 형식도 다양하면서 높은 답변 퀄리티를 가진 데이터 셋을 구축해 높은 성능의 라마를 만들 수 있었다.  
메타는 이런 결과를 바탕으로 **피상적인 정렬 가설**을 주장했다.  
이는 모델의 지식이나 능력은 사전 학습 단계에서 대부분 학습하고 정렬 데이터를 통해서는 답변의 형식이나 모델이 능력과 지식을 어떻게 나열할지 정도만 추가로 배우기 때문에
적은 정렬 데이터로도 사용자가 원하는 형태의 답변을 생성할 수 있다는 가설이다.
####  
3. 지시 데이터셋의 품질이 LLM의 성능에 얼마나 큰 영향을 미칠까??
마이크로소프트는 지시 데이터셋의 품질을 높이면 더 작은 데이터셋과 더 작은 모델로도 높은 성능을 달성할 수 있다고 주장했다.
마이크로소프트에서 공개한 파이썬 코드 생성 모델인 파이(Phi)의 학습 데이터를 구축할 때 공개된 코드 데이터셋을 그대로 사용하지 않고 선별했는데 그 이유는 다음과 같다.
* 외부 모듈이나 파일을 사용하기 때문에 하나의 코드의 코드 파일 자체에서 의미를 이해하기 어려운 경우가 많았다.
* 대부분의 파일은 의미 있는 연산보다는 보일러플레이트 코드나 설정파일이었다.
* 알고리즘 로직을 담고 있는 코드도 복잡하거나 제대로 문서화되지 않은 함수들 사이에 있어 의미를 파악하기 어려웠다.
* 특정 주제나 사례에 관련된 코드가 많아 특정 개념이나 스킬에 불균형한 데이터셋 분포를 보였다.
 
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_exam.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_exam.png)  
위의 사진을 예시로 들며 왼쪽은 함수마다 정확한 기능과 설명이 있어서 매우 읽기 좋은 독스트링으로 잘 정리돼있다.  
하지만 오른쪽은 그냥 처음 default 변수를 설정하는것으로 큰 의미를 가지고있지않은 코드이다.   
이런 데이터 중 왼쪽만 선별해서 사용했다는 뜻이겠네   
또한 GPT-3.5를 활용해 코드 예제 데이터셋(CodeExercieses)을 생성해 학습 데이터를 추가했다.   
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_exam2.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_exam2.png)      
위와 같이 처음에 무슨 코드인지 설명해주는 부분이 있고 아래쪽에 코드가 작성돼있다. 이 코드가 GPT가 생성한 독스트링이 잘된 코드이다.  
GPT가 이런 코드를 대량으로 생성해주니 좋은 데이터를 쉽게 만들 수 있었나보네  
그리고 데이터셋을 비교하며 정확도 결과를 보여준다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_re.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_re.png)    

## 메타와 마이크로소프트 기반 좋은 지시 데이터셋이 갖춰야 하는 조건을 정리  
1. 지시 데이터셋을 작은 규모로 구축하더라도 모델이 지시사항의 형식을 인식하고 답변하도록 만들 수 있다.
2. 지시사항이 다양한 형태이고 답변의 품질이 높을수록 모델의 답변 품질도 높아진다.
3. 학습 데이터의 품질을 높이기 위해 모델의 목적에 맞춰 학습 데이터의 교육적 가치를 판단하고 가치가 낮은것은 필터링한다.
4. 교재의 예제 데이터와 같은 고품질의 데이터를 학습 데이터에 추가하면 성능을 크게 높일 수 있다.
#####  
****
# RLHF랑 DPO 공부하기  
먼저 RLHF를 설명하기 위해 가독성을 평가하는 모델을 학습시키는 데이터셋을 예시로 설명한다.  
두개의 코드 A와 B가 있다. 그 중 더 가독성이 높은 코드를 선호 데이터 반대는 비선호 데이터라고한다.  
물론 이러한 평가는 비교 대상에 따라 달라진다 B와 C가 있을 때 B가 가독성이 좋다면 B가 선호 데이터가 된다.  
그럼 여기서 의문은 점수로 만들어 버리면 되는거 아니야? 이다.  
하지만 이러한 점수형태 데이터셋은 구하기도 어렵고 평가 기준이 애매해서 만들기도 어렵다.  
여러 코드를 바탕으로 어떤 코드가 더 가독성이 높은지를 선택해 선호 데이터셋을 구축하고 나면, 채점 모델이 선호 데이터에 비선호 데이터보다 높은 점수를 주도록 채점 모델을 학습시킨다.  
####
OpenAI가 이런 방식으로 학습을 사용했다. 초기 지도 미세 조정을 마친 LLM은 사용자의 요청에 맞춰 응답하기 때문에 사용자에게 결과적으로 해가 될 수 있는 정보를 제공했다.  
이런 부분을 줄이기 위해 위와같이 생성된 답변의 점수를 평가하는 리워드 모델을 만들었다.  
순서대로 정리하면,  
1. 지도 미세 조정을 마친 LLM에 지시사항을 입력해 여러 응답(A,B,C)을 생성한다.   
2. 레이블러가 응답을 비교해 더 좋다고 판단하는 순서를 정해 선호 데이터셋을 구축한다.   
3. 구축한 선호 데이터셋을 사용해 리워드 모델이 더 좋은 응답 순서대로 점수를 부여하도록 학습한다.

위와 같은 과정을 하나의 그림으로 표현했다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_reward.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_reward.png)    
####  
여기는 강화 학습 개념이 나오는데 애초 강화 학습 베이스기 때문에 설명이나 그림은 생략하고 필요한 부분만 넣습니다.  
####  
언어 모델은 다음 단어를 예측하는 방식으로 토큰을 하나씩 생성하는데 이 토큰 생성을 하나의 액션으로 볼 수있다.  
기존 강화학습은 한번 액션에 보상받고 학습하고 했다면  
언어모델에서는 모든 토큰을 생성해 완성된 문장을 가지고 한번에 보상을 받는다.  
이와 같은 방식으로 언어 모델은 생성한 문장의 점수가 높아지는 방향으로 학습한다.  
하지만 보상을 높게 받는 데만 집중하는 보상 해킹이 발생할 수 있다.  
그래서 PPO를 사용한다.  
PPO 설명을 너무 간단한 그림으로 수식 없이 표현해서 아쉽다...  
Clipping 방식을 거리로 비유해서 reward가 크더라도 거리가 너무 멀면 선택하지 않고 제한된 거리 안에서 가장 높은 reward를 내는 모델을 선택한다고 표현되어있다.    
처음 챗 GPT가 공개 되었을 때 세계에 엄청난 충격을 줬고 앞으로 LLM을 한다면 RLHF가 필수일거다 라고 생각했다.  
하지만 RLHF의 가장 어려운 점은 역시 보상 모델 학습이다.  
보상 모델의 성능이 좋이 않으면 LLM이 일관성 없는 점수를 통해 학습하게 되면서 제대로 학습이 되지 않는다. 따라서 성능이 높고 강인한(Robust)한 보상 모델을 만들어야 한다.   
또한 모델을 학습시킬 때 참고 모델, 학습 모델, 리워드 모델 3개의 모델이 필요하기 때문에 GPU를 많이 사용한다.  
마지막으로 PPO의 고질적인 문제인 하이퍼파라미터에 민감한 문제와 학습이 불안정하다는 점이 있다.  
이와 같은 연유로 강화 학습 없이 만든는 방법이 있을까? 고민한다.  
####  

## 기각 샘플(rejections sampling)  
기각 샘플링은 지도 미세 조정을 마친 LLM을 통해 여러 응답을 생성하고, 그중에서 리워드 모델이 가장 높은 점수를 준 응답을 모아 다시 지도 미세 조정을 수행한다.  
강화 학습을 사용하지 않기 때문에 학습이 비교적 안정적이고 간단하고 직관적인 방법인데도 효과가 좋아 많이 활용한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_reject.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_reject.png)    

먼저 왼쪽아래부터 시작한다. 자기 지도 학습을 통해 원하는 기본 모델을 학습한다. 다음으로 지도 미세 조정을 거쳐서 챗 모델로 학습시킨다.  
왼쪽 위에서 챗 모델이 더 안전하고 사람들에게 도움이 될 수 있도록 사람의 피드백을 반영한 선호 데이터셋을 구축하고   
마지막으로 RLHF를 통해 사람들이 선호나는 답변을 하도록 학습한다.  
이렇게 보상 모델 없이 사람이 직접 샘플링을 해버린 데이터로 학습을 진행하므로 사람의 선호를 학습하는데 효과적이고 안정적으로 학습할 수 있다.  
(여기부터는 사담)  
이해 안되는 부분은 왼쪽 위에 결국 보상 모델들이 있는데 저건 뭐지? 사람이 임의로 보상을 저렇게 만들어준다는건가?  
기각 샘플링을 검색해보면 확률쪽 이론이 나와서 오히려 그건 이해했는데 이거랑은 조금 다르다.  
PPO Loss에 결국 r이 들어가니까 저 보상 함수 값들을 사용한다는거 같은데 두개의 보상 모델이면 가중치 줘서 합쳐버리나?  
이러면 진짜 한번 에피소드 도는것도 오래걸리겠다.   
####  

## DPO(Direct Preference Optimization)  
기존에는 좋은 보상 모델을 만들고 관리하는 것도 쉽지 않은 일이고 보상 모델을 만든다고 하더라도 보상 모델에 강화 학습으로 사람의 선호를 반영한다는 것도 쉽지 않았다.  
DPO 선호 데이터셋을 직접 언어 모델에 학습시킨다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_diff.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_diff.png)    

앞에서 언어 모델은 다음에 올 토큰의 확률을 예측한다고 했다. 그리고 우리는 특정 입력에 대해 어떤 응답이 선호되는지 수집한 선호 데이터셋이 있다.  
DPO는 이 두가지를 결합해 아래 그림과 같이 선호 데이터를 직접 학습 시킨다.  

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_dpo.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/4_dpo.png)   

선호 데이터셋에 있는 데이터에 대해 이 과정을 반복하면, 언어 모델은 점차 선호 데이터를 자주 생성하고 비선호 데이터를 드물게 생성해, 선호 데이터에 가까운 결과를 생성하는 언어 모델이 된다.  
DPO는 보상 모델이 없고 강화 학습도 없다. 따라서 훨씬 쉽고 빠르게 모델에 사람의 선호를 반영할 수 있다.  
DPO를 가장 직관적으로 설명한 문구는 논문에 있는 "당신의 언어 모델은 보상 모델이기도 하다"라고 할 수 있다.  

####
***  
# 정리  
전체적인 정리
1. 대용량의 말뭉치를 언어 모델링 방식으로 사전 학습하고 지시사항과 그에 대한 응답으로 구성된 지시 데이터셋으로 지도 미세 조정을 수행한다.
2. 지도 미세 조정을 거치면 LLM은 사용자의 지시사항에 따라 응답할 수 있게 된다.
3. 사람들이 선호하는 방식으로 답변을 생성하고, 더 안전한 모델이 될 수 있도록 선호 데이터셋을 활용해 모델을 조정하는 방법도 알아봤다.
####
선호 학습을 수행하는 방법은 크게 강화 학습 사용 유무로 나눴다.  
OpenAI는 챗GPT를 만들 때 보상 모델과 강화 학습을 사용했다. 하지만 강화 학습은 사용하기 까다롭다는 문제가 있었다.  
기각 샘플링은 강화 학습을 사용하지 않고 보상 모델로부터 높은 점수를 받은 결과로 지도 미세 조정을 수행해 학습을 안정적으로 만들어준다.  
DPO는 강화 학습은 물론 보상 모델도 필요 없어 2024년 기준 가장 많이 쓰는 선호 학습 방법이다.


