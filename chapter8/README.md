# sLLM 서빙하기  
앞에서는 성능을 희생해 효유럿ㅇ을 높였다면 여기서는 동일한 여산을 수행해 성능 하락이 없으면서도 추론 속도를 높이는 방법들을 알아본다.  
언어 모델은 배치 데이터에 대해 한 번에 하나씩 토큰을 생성하면서 추론을 수행한다.  
그러다 보면 입력 데이터에 따라 생성이 끝난 데이터와 진행중인 데이터가 구분되며 대기 시간이 길어진다. 이를 위해 효율적인 배치 데이터 관리 전략을 살펴본다.  
2장에서 트랜스 포머를 공부하며 어텐션 연산은 무겁워서 시간이 오래 걸리는 연산이라고 배웠다.
해당 연산을 효율적으로 하기 위해 아랫 목록을 배운다.  
1. 어텐션 연산을 효율적으로 수행할 수 있는 **플래시 어텐션**
2. KV 캐시 메모리를 효율적으로 관리해 배치 크기를 크게 키울 수 있는 **페이지어텐션**
효율적인 추론 전략들  
1. GPU에서 자주 사용되는 여산을 하나로 합쳐 속도를 높이는 **커널 퓨전**
2. 다음 단어를 예측할 때 쉬운 단어는 작은 모델을 사용하고 어려운 단어는 큰 모델을 사용해 속도를 높이는 **추측 디코딩**
3. 셀프 어텐션 연산에서 입력 토큰 사이의 상대적 위치 정보를 추가로 넣어주는 **상대적 위치 인코딩**



####
***
## 효율적인 배치 전략  
입력 데이터를 추론할 때 가능한 한 번에 많은 데이터를 받아 처리량을 높이는 것이 GPU를 효율적으로 사용하는 방법이다.  
하지만 한 번에 하나의 토큰을 생성하고 입력에 따라 몇 개의 토큰을 추가로 생성할지 모르기 때문에 배치 전략을 세우는데 고려 사항이 많다.  
해당 섹션에서는 다음과 같은 전략을 살펴본다.  
1. 가장 기본적인 배치 전략인 일반 배치
2. 비슷한 시간대에 들어온 요청을 하나의 배치로 묶는 동적 배치
3. 배치 데이터에서 생성이 완료된 데이터를 제거하고 새로운 데이터를 추가하는 연속 배치

### 일반 배치(정적 배치)  
가장 기본적인 방식으로 입력으로 들어온 N개의 데이터의 모든 추론이 끝날 때까지 기다리는 방식이다. 이를 일반 배치(naive batching) or 정적 배치(static batching)으로 부른다.
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_1.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_1.png)   

위 그림처럼 첫번째 s1,s3은 입력 토큰이 3개고 s2는 2개, s4는 4개다 그리고 파란색은 추론하며 생긴 토큰이다.  
오른쪽은 모든 추론이 끝난 상태를 나타낸다.  
여기서 생기는 문제는 두개이다.  
1. s3는 생성이 생성이 종료된 이후에 다른 데이터의 추론을 기다리느라 결과를 반환하지 못하고 기다리고 있다.
2. 생성이 일찍 끝난 문장이 있다면 결과적으로 배치 크기가 작아져 GPU를 효율적으로 사용하지 못한다.

### 동적 배치  
동적 배치(dynamic batching)는 비슷한 시간대에 들어온 요청을하나의 배치로 묶어 배치크기를 키우는 전략이다.  
사용자가 언제 요청을 할지모르기 때문에 요청을 순서대로 수행하면 처음 요청은 빠르게 답변을 받지만 다음 답변은 앞에 요청이 끝날 때까지 기다려야한다.  
이를위해 요청이 오면 바로 수행하는 것이 아닌 일정시간 대기하면서 다른 사용자의 요청을 기다렸다가 한 번에 처리한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_2.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_2.png)   

위 그림 왼쪽을 보면 요청이 들어올 때마다 추론을 수행하면 지연 시간은 짧지만 GPU를 효율적으로 사용하지 못한다.  
또한 추론에 1ms 이상 소요된다면 두번째 요청의 지연 시간은 길어지게된다.  
하지만 오른쪽처럼 동적 배치를 사용한다면 첫 번째와 두 번째의 요청은 1~2ms 지연되지만 전체적인 처리량은 증가한다.  
동적 배치를 사용하면 온라인 서빙(online serving)에서 배치 크기를 키워 처리량을 높일 수 있다.  
하지만 해당 전략은 입력 데이터의 생성하는 토큰 길이 차이에 따른 배치 크기 문제는 여전히 존재한다.  

### 연속 배치  
연속 배치(continuous batching)는 하나의 토큰 생성이 끝날 때마다 생성이 끝난 문장은 제거하고 새로운 문장을 추가한다.  
그러면 생성이 끝나 다른 문장을 기다리는 대기 시간이 길어지는 문제를 해결하고 GPU를 효율적으로 사용할 수 있다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_3.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_3.png)   
하지만 새로운 데이터를 배치에 추가할 때 고려해야 할 사항이 있다.  
앞에 7장에서 언어 모델의 추론 과정을 보면 입력 프롬프트를 병렬로 처리하는 사전 연산과 한 토큰씩 생성하는 디코딩으로 나눌 수 있다고 배웠다.  
사전 연산과 디코딩은 처리 방식이 다르기 때문에 한번의 토큰 생성이 끝날 때마다 새로운 문장을 배치에 추가하지는 않고 처리 중인 문장과 대기 중인 문장의 비율을 보고 조건을 달생했을 때 추가하기도 한다.  
**연속 배치 방식은 다양한 LLM 서빙 프레임 워크에서 사용되고 있기 때문에 잘 알아둘 필요가 있다.**  
***
## 효율적인 트랜스포머 연산  
트랜스포머에서 셀프 어텐션 연산은 쿼리와 키 벡터 사이의 관련도를 계산해 새롭게 토큰 임베딩을 조정하기 때문에 성능이 높지만 많은 연산을 필요로 했다.  
이번 절에서는 어텐션 연산을 개선해 동일한 계산을 수행하면서도 메모리 사용량을 줄이고 속도를 높인 플래시어텐션을 살펴본다.  
####  
트랜스포머에서 위치 정보를 넣어주기위해 절대적 위치 인코딩을 사용했었다. 이는 학습 데이터보다 긴 입력 데이터가 들어올 경우 성능이 크게 저하된다는 단점이 있었다.  
이를 개선하기 위해 토큰 사이의 상대적 위치 정보를 추가하는 상대적 위치 인코딩(relative positional encoding)이 개발됐다.  
####    
### 플래시 어텐션(Flash Attention)  
플래시 어텐션은 트랜스포머가 더 긴 시퀀스를 처리하도록 만들기 위해 개발됐다.  
연산량이 학습 과정에서 시퀀스 길이의 제곱에 비례하고 추론에서는 길이에 비례해서 증가하기 때문에 긴 시퀀스를 처리하는데 어려움이 있었다.  
플래시 어텐션은 어텐션 연산 과정을 변경해 학습 과정에서 필요한 메모리를 시퀀스 길이에 비례하도록 개선했다.  
트랜스포머 연산은 쿼리와 키 벡터를 곱하는 과정에서 많은 메모리를 사용한다. 밑에 그림을 보며 설명한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_4.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_4.png)   
그림을 보면 입력과 출력보다 중간 연산 과정에서 필요한 행렬의 크기가 제곱으로 큰것을 확인 할 수있다.  
어텐션 연산에 드는 시간을 측정하면 마스크, 소프트맥스, 드롭아웃 처리에 드는 시간이 행렬 곱셈에 드는 시간보다 더 길다.  
하지만 연산량 자체는 행렬 곱셈이 더 크다. 이것으로 시간이 오래 걸리는 작업은 어텐션 행렬에 취하는 연산임을 알 수 있다.  
따라서 큰 메모리를 사용하는 세 연산이 많은 연산량이 필요한 행렬 곱셈보다 오래 걸린다는 사실에서, 어텐션 연산이 오래걸리는 이유는 GPU에서 메모리를 읽고 쓰는 데 오래 시간이 걸리기 때문임을 알 수 있다.  
일반적인 GPU의 구조를 보면 아래 그림과 같다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_5.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_5.png)   
1. 데이터를 빠르게 이동시킬 수 있는 SRAM(Static Ramdom Access Memory)
2. 데이터 이동 속도가 느린 고대역폭 메모리(High Bandwidth Memory, HBM)
SRAM은 빠르지만 메모리 크기가 작아 대부분의 읽기 쓰기 작업은 HBM에서 이뤄진다.
따라서 어텐션 행렬은 크기가 크기 때문에 SRAM에서 처리할 수 없고 저장 공간이 큰 HBM에 쓰고 다시 읽으면서 연산을 수행해야한다.
하지만 HBM 데이터 이동 속도가 느리기 때문에 큰 어텐션 행렬을 쓰고 읽는데 오랜 시간이 걸린다.
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_6.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_6.png)  
책에 나와 있는 그림을 내가 해석한대로 그려봤다.
먼저 일정 크기의 블록 단위로 계산을 실행한다. 이떄 블록의 크기는 SRAM 메모리 크기에 맞춰서 만들어준다.
이 블록에 해당하는 Q와 K 부분을 복사해서 행렬 곱셈을 통해 블록 크기의 행렬을 만든다. 이때 크기가 SRAM에 들어가므로 SRAM에서 모든 연산을 전부 수행한다.
그리고 연산한 블록을 꺼내와 해당하는 V 부분을 복사해와서 곱해주면 최종 결과를 만들 수 있다. 이 최종 결과를 HBM에 저장한다. SRAM은 속도가 빠르니까 HBM으로 보내는 이동 시간이 굉장히 줄어든다.
그리고 책에 소프트맥스는 분모항에 모든 값의 합이 들어가는데 어떻게 블록 처리로 계산할거냐? 라는 질문에 친절한 답이 있으니 이건 참고하자.
역전파를 계산할 때 순전파에서 계산한 N x N 행렬의 값이 필요한데 플래시 어텐션은 역전파 과정에서 다시 순전파를 계산하는 방식으로 해결한다.
이러면 당연히 연산량이 증가하지만 앞에서 봤듯이 시간이 오래 걸리는 소프트맥스,드롭아웃,마스크 과정은 하지 않으므로 오히려 시간이 줄어든다.
최종 결과를 보면 연산량은 66.6 에서 75.2로 증가하지만 실행 시간은 40.3에서 4.4로 1/6정도로 크게 감소한다.
지금까지 GPU 메모리에 데이터를 읽고 쓰는 작업(IO)를 줄여 메모리 사용량을 줄이고 속도를 높인 플래시어텐션에 대해 공부했다.
다음으로 한 단계 더 속도를 높인 플래시어텐션 2를 살펴본다.
### 플래시 어텐션 2  
플래시어텐션2는 기존에 비해 2배정도 속도를 개선했다. 버전 2에서 개선한 부분은 크게 두 가지이다.  
1. 행렬 곱셈이 아닌 연산 줄이기
2. 시퀀스 길이 방향의 병렬화 추가

첫 번째 개선점은 어렵지 않았다.  
연산 과정에서 행렬 곱셈이 아닌 연산을 줄였다. A100 GPU 기준 FP16 또는 BF16 행렬 곱셈 연산은 최대 312TFLOPS까지 가능하지만 FP32인 비 행렬 곱셈 연산은 19.5TFLOPS밖에 처리 못한다.  
이는 단순 계산으로도 16배 속도 차이가 나는데 비용 측면에서 보자면 비 행렬 곱셈 연산이 행렬 곱셈 연산보다 16배 비싸다는 의미가 된다.  
따라서 플래시 어텐션2는 어텐션 연산 중에 발생하는 비 행렬 곱셈 연산을 최대한 효율적으로 수행하는 방식으로 속도를 향상했다.  
####  
####  
다음은 기존에는 (배치크기 X 어텐션 헤드 수) 만큼의 스레드 블록(thread block)으로 병렬 처리를 했는데 시퀀스 길이 방향으로 병렬화를 추가했다.  
GPU의 가장 작은 계산 단위는 스레드이고 GPU에서는 스레드의 모음인 스레드 블록 단위로 병렬 처리를 수행한다.  
GPU에는 여러 개의 스트리밍 멀티프로세서 SM이 있는데 하나의 스레드 블록은 하나의 SM에 배정돼 처리된다.  
또한 워프(warp)는 GPU에서 더 효율적인 연산을 위해 보통 32개의 스레드를 하나의 명령으로 실행하는 단위를 말한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_7.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_7.png)  

GPU를 효율적으로 사용하기 위해서는 충분한 수의 스레드 블록ㅇ 있어야 하는데, 배치 크기가 작거나 어텐션 헤드 수가 작은 경우 SM을 충분히 활용하지 못 할 수 있다.  
예를들어, A100 GPU에는 108개의 SM이 있는데 헤드 수가 32이고 배치 크기가 1~2로 잡힌다면 72개의 SM만 사용하므로 상당 수의 SM이 놀게된다.  
따라서 플래시 어텐션 2는 시퀀스의 길이 방향으로 여러 개의 묶음으로 나눠 사용하는 스레드 블록 수를 늘린다.(그림으로 이해하는게 좋은데 그리기 어려워서 책 참고 p.269)  



