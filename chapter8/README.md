# sLLM 서빙하기  
앞에서는 성능을 희생해 효유럿ㅇ을 높였다면 여기서는 동일한 여산을 수행해 성능 하락이 없으면서도 추론 속도를 높이는 방법들을 알아본다.  
언어 모델은 배치 데이터에 대해 한 번에 하나씩 토큰을 생성하면서 추론을 수행한다.  
그러다 보면 입력 데이터에 따라 생성이 끝난 데이터와 진행중인 데이터가 구분되며 대기 시간이 길어진다. 이를 위해 효율적인 배치 데이터 관리 전략을 살펴본다.  
2장에서 트랜스 포머를 공부하며 어텐션 연산은 무겁워서 시간이 오래 걸리는 연산이라고 배웠다.
해당 연산을 효율적으로 하기 위해 아랫 목록을 배운다.  
1. 어텐션 연산을 효율적으로 수행할 수 있는 **플래시 어텐션**
2. KV 캐시 메모리를 효율적으로 관리해 배치 크기를 크게 키울 수 있는 **페이지어텐션**
효율적인 추론 전략들  
1. GPU에서 자주 사용되는 여산을 하나로 합쳐 속도를 높이는 **커널 퓨전**
2. 다음 단어를 예측할 때 쉬운 단어는 작은 모델을 사용하고 어려운 단어는 큰 모델을 사용해 속도를 높이는 **추측 디코딩**
3. 셀프 어텐션 연산에서 입력 토큰 사이의 상대적 위치 정보를 추가로 넣어주는 **상대적 위치 인코딩**



####
***
## 효율적인 배치 전략  
입력 데이터를 추론할 때 가능한 한 번에 많은 데이터를 받아 처리량을 높이는 것이 GPU를 효율적으로 사용하는 방법이다.  
하지만 한 번에 하나의 토큰을 생성하고 입력에 따라 몇 개의 토큰을 추가로 생성할지 모르기 때문에 배치 전략을 세우는데 고려 사항이 많다.  
해당 섹션에서는 다음과 같은 전략을 살펴본다.  
1. 가장 기본적인 배치 전략인 일반 배치
2. 비슷한 시간대에 들어온 요청을 하나의 배치로 묶는 동적 배치
3. 배치 데이터에서 생성이 완료된 데이터를 제거하고 새로운 데이터를 추가하는 연속 배치

### 일반 배치(정적 배치)  
가장 기본적인 방식으로 입력으로 들어온 N개의 데이터의 모든 추론이 끝날 때까지 기다리는 방식이다. 이를 일반 배치(naive batching) or 정적 배치(static batching)으로 부른다.
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_1.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_1.png)   

위 그림처럼 첫번째 s1,s3은 입력 토큰이 3개고 s2는 2개, s4는 4개다 그리고 파란색은 추론하며 생긴 토큰이다.  
오른쪽은 모든 추론이 끝난 상태를 나타낸다.  
여기서 생기는 문제는 두개이다.  
1. s3는 생성이 생성이 종료된 이후에 다른 데이터의 추론을 기다리느라 결과를 반환하지 못하고 기다리고 있다.
2. 생성이 일찍 끝난 문장이 있다면 결과적으로 배치 크기가 작아져 GPU를 효율적으로 사용하지 못한다.

### 동적 배치  
동적 배치(dynamic batching)는 비슷한 시간대에 들어온 요청을하나의 배치로 묶어 배치크기를 키우는 전략이다.  
사용자가 언제 요청을 할지모르기 때문에 요청을 순서대로 수행하면 처음 요청은 빠르게 답변을 받지만 다음 답변은 앞에 요청이 끝날 때까지 기다려야한다.  
이를위해 요청이 오면 바로 수행하는 것이 아닌 일정시간 대기하면서 다른 사용자의 요청을 기다렸다가 한 번에 처리한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_2.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_2.png)   

위 그림 왼쪽을 보면 요청이 들어올 때마다 추론을 수행하면 지연 시간은 짧지만 GPU를 효율적으로 사용하지 못한다.  
또한 추론에 1ms 이상 소요된다면 두번째 요청의 지연 시간은 길어지게된다.  
하지만 오른쪽처럼 동적 배치를 사용한다면 첫 번째와 두 번째의 요청은 1~2ms 지연되지만 전체적인 처리량은 증가한다.  
동적 배치를 사용하면 온라인 서빙(online serving)에서 배치 크기를 키워 처리량을 높일 수 있다.  
하지만 해당 전략은 입력 데이터의 생성하는 토큰 길이 차이에 따른 배치 크기 문제는 여전히 존재한다.  

### 연속 배치  
연속 배치(continuous batching)는 하나의 토큰 생성이 끝날 때마다 생성이 끝난 문장은 제거하고 새로운 문장을 추가한다.  
그러면 생성이 끝나 다른 문장을 기다리는 대기 시간이 길어지는 문제를 해결하고 GPU를 효율적으로 사용할 수 있다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_3.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_3.png)   
하지만 새로운 데이터를 배치에 추가할 때 고려해야 할 사항이 있다.  
앞에 7장에서 언어 모델의 추론 과정을 보면 입력 프롬프트를 병렬로 처리하는 사전 연산과 한 토큰씩 생성하는 디코딩으로 나눌 수 있다고 배웠다.  
사전 연산과 디코딩은 처리 방식이 다르기 때문에 한번의 토큰 생성이 끝날 때마다 새로운 문장을 배치에 추가하지는 않고 처리 중인 문장과 대기 중인 문장의 비율을 보고 조건을 달생했을 때 추가하기도 한다.  
**연속 배치 방식은 다양한 LLM 서빙 프레임 워크에서 사용되고 있기 때문에 잘 알아둘 필요가 있다.**  
***
## 효율적인 트랜스포머 연산  
트랜스포머에서 셀프 어텐션 연산은 쿼리와 키 벡터 사이의 관련도를 계산해 새롭게 토큰 임베딩을 조정하기 때문에 성능이 높지만 많은 연산을 필요로 했다.  
이번 절에서는 어텐션 연산을 개선해 동일한 계산을 수행하면서도 메모리 사용량을 줄이고 속도를 높인 플래시어텐션을 살펴본다.  
####  
트랜스포머에서 위치 정보를 넣어주기위해 절대적 위치 인코딩을 사용했었다. 이는 학습 데이터보다 긴 입력 데이터가 들어올 경우 성능이 크게 저하된다는 단점이 있었다.  
이를 개선하기 위해 토큰 사이의 상대적 위치 정보를 추가하는 상대적 위치 인코딩(relative positional encoding)이 개발됐다.  
####    
### 플래시 어텐션(Flash Attention)  
플래시 어텐션은 트랜스포머가 더 긴 시퀀스를 처리하도록 만들기 위해 개발됐다.  
연산량이 학습 과정에서 시퀀스 길이의 제곱에 비례하고 추론에서는 길이에 비례해서 증가하기 때문에 긴 시퀀스를 처리하는데 어려움이 있었다.  
플래시 어텐션은 어텐션 연산 과정을 변경해 학습 과정에서 필요한 메모리를 시퀀스 길이에 비례하도록 개선했다.  
트랜스포머 연산은 쿼리와 키 벡터를 곱하는 과정에서 많은 메모리를 사용한다. 밑에 그림을 보며 설명한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_4.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_4.png)   
그림을 보면 입력과 출력보다 중간 연산 과정에서 필요한 행렬의 크기가 제곱으로 큰것을 확인 할 수있다.  
어텐션 연산에 드는 시간을 측정하면 마스크, 소프트맥스, 드롭아웃 처리에 드는 시간이 행렬 곱셈에 드는 시간보다 더 길다.  
하지만 연산량 자체는 행렬 곱셈이 더 크다. 이것으로 시간이 오래 걸리는 작업은 어텐션 행렬에 취하는 연산임을 알 수 있다.  
따라서 큰 메모리를 사용하는 세 연산이 많은 연산량이 필요한 행렬 곱셈보다 오래 걸린다는 사실에서, 어텐션 연산이 오래걸리는 이유는 GPU에서 메모리를 읽고 쓰는 데 오래 시간이 걸리기 때문임을 알 수 있다.  
일반적인 GPU의 구조를 보면 아래 그림과 같다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_5.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_5.png)   
1. 데이터를 빠르게 이동시킬 수 있는 SRAM(Static Ramdom Access Memory)
2. 데이터 이동 속도가 느린 고대역폭 메모리(High Bandwidth Memory, HBM)

SRAM은 빠르지만 메모리 크기가 작아 대부분의 읽기 쓰기 작업은 HBM에서 이뤄진다.  
따라서 어텐션 행렬은 크기가 크기 때문에 SRAM에서 처리할 수 없고 저장 공간이 큰 HBM에 쓰고 다시 읽으면서 연산을 수행해야한다.  
하지만 HBM 데이터 이동 속도가 느리기 때문에 큰 어텐션 행렬을 쓰고 읽는데 오랜 시간이 걸린다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_6.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_6.png)  
책에 나와 있는 그림을 내가 해석한대로 그려봤다.  
먼저 일정 크기의 블록 단위로 계산을 실행한다. 이떄 블록의 크기는 SRAM 메모리 크기에 맞춰서 만들어준다.  
이 블록에 해당하는 Q와 K 부분을 복사해서 행렬 곱셈을 통해 블록 크기의 행렬을 만든다. 이때 크기가 SRAM에 들어가므로 SRAM에서 모든 연산을 전부 수행한다.  
그리고 연산한 블록을 꺼내와 해당하는 V 부분을 복사해와서 곱해주면 최종 결과를 만들 수 있다. 이 최종 결과를 HBM에 저장한다. SRAM은 속도가 빠르니까 HBM으로 보내는 이동 시간이 굉장히 줄어든다.  
그리고 책에 소프트맥스는 분모항에 모든 값의 합이 들어가는데 어떻게 블록 처리로 계산할거냐? 라는 질문에 친절한 답이 있으니 이건 참고하자.  
역전파를 계산할 때 순전파에서 계산한 N x N 행렬의 값이 필요한데 플래시 어텐션은 역전파 과정에서 다시 순전파를 계산하는 방식으로 해결한다.  
이러면 당연히 연산량이 증가하지만 앞에서 봤듯이 시간이 오래 걸리는 소프트맥스,드롭아웃,마스크 과정은 하지 않으므로 오히려 시간이 줄어든다.  
최종 결과를 보면 연산량은 66.6 에서 75.2로 증가하지만 실행 시간은 40.3에서 4.4로 1/6정도로 크게 감소한다.  
지금까지 GPU 메모리에 데이터를 읽고 쓰는 작업(IO)를 줄여 메모리 사용량을 줄이고 속도를 높인 플래시어텐션에 대해 공부했다.  
다음으로 한 단계 더 속도를 높인 플래시어텐션 2를 살펴본다.  
### 플래시 어텐션 2  
플래시어텐션2는 기존에 비해 2배정도 속도를 개선했다. 버전 2에서 개선한 부분은 크게 두 가지이다.  
1. 행렬 곱셈이 아닌 연산 줄이기
2. 시퀀스 길이 방향의 병렬화 추가

첫 번째 개선점은 어렵지 않았다.  
연산 과정에서 행렬 곱셈이 아닌 연산을 줄였다. A100 GPU 기준 FP16 또는 BF16 행렬 곱셈 연산은 최대 312TFLOPS까지 가능하지만 FP32인 비 행렬 곱셈 연산은 19.5TFLOPS밖에 처리 못한다.  
이는 단순 계산으로도 16배 속도 차이가 나는데 비용 측면에서 보자면 비 행렬 곱셈 연산이 행렬 곱셈 연산보다 16배 비싸다는 의미가 된다.  
따라서 플래시 어텐션2는 어텐션 연산 중에 발생하는 비 행렬 곱셈 연산을 최대한 효율적으로 수행하는 방식으로 속도를 향상했다.  
####  
####  
다음은 기존에는 (배치크기 X 어텐션 헤드 수) 만큼의 스레드 블록(thread block)으로 병렬 처리를 했는데 시퀀스 길이 방향으로 병렬화를 추가했다.  
GPU의 가장 작은 계산 단위는 스레드이고 GPU에서는 스레드의 모음인 스레드 블록 단위로 병렬 처리를 수행한다.  
GPU에는 여러 개의 스트리밍 멀티프로세서 SM이 있는데 하나의 스레드 블록은 하나의 SM에 배정돼 처리된다.  
또한 워프(warp)는 GPU에서 더 효율적인 연산을 위해 보통 32개의 스레드를 하나의 명령으로 실행하는 단위를 말한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_7.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_7.png)  

GPU를 효율적으로 사용하기 위해서는 충분한 수의 스레드 블록ㅇ 있어야 하는데, 배치 크기가 작거나 어텐션 헤드 수가 작은 경우 SM을 충분히 활용하지 못 할 수 있다.  
예를들어, A100 GPU에는 108개의 SM이 있는데 헤드 수가 32이고 배치 크기가 1~2로 잡힌다면 72개의 SM만 사용하므로 상당 수의 SM이 놀게된다.  
따라서 플래시 어텐션 2는 시퀀스의 길이 방향으로 여러 개의 묶음으로 나눠 사용하는 스레드 블록 수를 늘린다.(그림으로 이해하는게 좋은데 그리기 어려워서 책 참고 p.269)  
####
## 상대적 위치 인코딩  
초반 트랜스포머에서는 위치 인코딩은 절대적인 값을 더해서 위치 정보를 만들어 주었고 그 식은 아래와 같다.  
현재 토큰의 위치가 짝수이면 sin을 홀수이면 cos을 이용해 값을 만들어줬다.
$PE_{(\text{pos},2i)}=sin(\text{pos}/10000^{2i/d_{\text{model}}})$  
$PE_{(\text{pos},2i+1)}=cos(\text{pos}/10000^{2i/d_{\text{model}}})$   
이렇게 수식을 이용해 절대 위치 인코딩을 사용하기도 하지만 학습 파라미터로 만들어서 학습 과정에서 같이 학습하는 경우도 있다.  
생각해보면 학습이 끝나면 결국 정해진 파라미터를 더해주기때문에 이것도 절대적 위치 인코딩에 해당한다.  
절대적 위치 인코딩의 문제는 학습 데이터와 비슷한 입력 데이터에서는 잘 작동하지만 더 긴 입력이 들어오면 품질이 빠르게 떨어진다는 문제가 있다.  
이를 해결하기 위해 토큰과 토큰 사이의 상대적인 위치 정보를 추가하는 상대적 위치 인코딩 방식이 활발히 연구됐다.  
####  
검은 고양이가 밥을 먹고 물을 마신다.  
어제 아침에 길에서 검은 고양이가  
####  
위와 같은 예시가 있을 때 절대적 위치 인코딩은 검은 고양이가 라는 토큰에 다른 임베딩 값을 추가한다.  
하지만 상대적 위치 인코딩의 경우 검은 고양이가 라는 토큰이 어디에 등장하는게 중요한게 아니라 검은 다음의 고양이가 라는 토큰이 등장했다는 사실이 더 중요하다고 생각한다.  
따라서 상대적 위치 인코딩은 검은 과 고양이가 가 얼마나 떨어져 있는지를 모델에 입력해서 토큰의 상대적 위치에 따라 입력 문장의 의미가 어떻게 달라지는지 학습할 수 있도록 한다.  
대표적인 상대적 위치 인코딩 방식으로는  
1. RoPE(Rotary Positional Encoding)
2. ALiBi(Attention with Linear Biases)

가 있다.  

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_8.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_8.png)   
RoPE 같은 경우는 회전 각을 이용해서 위치 인코딩을 해준다.  
왼쪽 끝에는 맨 처음 검은 과 고양이가 라는 토큰의 임베딩이라고 가정한다.  
첫 예제에서는 검은 은 첫 번째 위치이므로 $\theta$를 회전하고 고양이가 는 2 번째 위치이므로 2$\theta$를 회전한다.  
다른 예제에서는 검은 은 4 번째 위치이므로 4$\theta$를 회전하고 고양이가 는 5 번째 위치이므로 5$\theta$를 회전한다.  
두 토큰의 위치 인코딩을 적용한 결과 임베딩은 서로 다르지만 둘 다 한 칸 사이에 떨어져 있으므로 두 임베딩 사이의 각도는 동일하다.  
즉, RoPE를 사용할 경우 토큰 사이의 위치 정보가 두 임베딩 사이의 각도를 통해 모델에 반영된다.  
RoPE는 간단하고 직관적인 위치 인코딩 방식으로 모델이 학습 데이터보다 더 긴 입력을 처리하는 능력을 높였다.  
####

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_9.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_9.png)    
ALiBi는 위 그림과 같이 왼쪽의 q와 k 벡터를 곱한 어텐션 행렬에 오른쪽에서 왼쪽으로 갈수록 더 작은 값을 더하는 방식을 사용한다.  
상대적 위치 인코딩을 사용하면 입력 토큰이 길어져도 사인파 위치 인코딩에 비해 성능이 빠르게 나빠지지 않는다.  

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_10.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_10.png)    

위 그림에서 왼쪽은 입력 토큰이 512개 일 때 x축 만큼의 입력 토큰을 넣었을 때 성능이 얼마나 떨어지는지 나타내는 그래프이다.  
y축은 펄플렉시티로 새로운 토큰을 예측할 때 확신의 정도를 곱해 역수를 취한 성능 지표로 낮을수록 좋은 값이다.  
직관적으로 다음 토큰의 불확실성을 의미하는데 다음 토큰을 더 확신을 가지고 예측할경우 불확실성이 낮기 때문에 낮은 값이 좋은 것이다.  
RoPE의 경우 사인파 방식에 비해 천천히 성능이 떨어지고 ALiBi는 입력이 길어져도 성능이 거의 떨어지지 않는다.   

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_11.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_11.png)    

위 그림을 보면 각 위치 인코딩을 사용했을 때 속도를 비교할 수 있다.  
ALiBi의 경우 정해진 값을 더해주는 것이기 때문에 별도로 처리 시간을 추가하지 않으므로 빠르다.  
하지만 RoPE의 경우 토큰 임베딩을 회전하는 처리가 추가되므로 속도가 느려진다.  

## 효율적인 추론 전략  
1. GPU에서 반복적으로 수행하는 연산을 하나로 묶어 더 효율적으로 처리하는 커널 퓨전
2. KV 캐시는 추론을 빠르게 하는 만큼 GPU 메모리를 잡아먹는데 이를 효율적으로 관리하기 위한 페이지 어텐션
3. 언어 모델 추론 과정에서 비교적 명확하고 쉬운 다음 단어는 더 작고 효율적인 모델을 사용하고 어려운건 더 크고 성능 좋은 모델로 수행하는 추측 디코딩

####   
### 커널 퓨전  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_12.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_12.png)    
GPU에서 연산은 커널 단위로 실행된다. 하지만 연산을 위해서는 전후에 추가적인 작업을 위한 오버헤드가 발생한다.  
대표적인 오버헤드는 HBM에서 데이터를 읽어오고 연산 결과를 쓰는 작업이 있다.  
그렇다면 연산이 많을수록 커널이 많아지고 오버헤드도 많아져서 연산 시간이 길어진다.  
따라서 반복적으로 수행하는 연산에 대해서는 각 커널 단위로 분리해서 실행하는 것 보다 연산을 하나로 묶어 오버헤드를 줄이는 커널 퓨전을 사용해 연산 과정을 효율적으로 만든다.  
앞에서 우리는 플래시 어텐션을 배웠는데 결국 큰 행렬을 읽고 쓰는데 시간이 걸려 블록 단위로 연산하는 것이였다. 이 큰 행렬을 읽고 쓰는데 걸리는 시간이 오버헤드다.  
그래서 커널 퓨전을 사용해 시간이 오래 걸리던 마스크,소프트맥스,드롭아웃 연산을 하나로 묶어버려 연산 시간을 대폭 줄였다.  

### 페이지 어텐션  

기존의 KV 캐시는 앞으로 사용할 수도 있는메모리를 미리 잡아두면서 GPU 메모리를 많이 낭비한다는 문제가 있었다.  
만약 사용자가 최대 토큰 길이를 2048로 잡았는데 생성한 토큰은 겨우 10개 뿐이라면 나머지 2038개의 메모리는 결국 사용되지 않고 자리만 차지하게된다.  
이러한 메모리 낭비는 '연속적인 물리 메모리'를 사용하기 위해 미리 메모리를 준비하기 때문에 발생한다.  
즉, GPU 메모리에서 KV 캐시를 토큰 순서에 따라 바로 옆에 저장할 수 있도록 미리 예약해 두는 것이다.  
페이지 어텐션에서는 운영체제의 가상 메모리 개념을 빌려와 중간에서 논리적 메모리와 물리적 메모리를 연결하는 블록 테이블을 관리해서 실제로는  
물리적으로 연속된 메모리를 사용하지 않으면서도 논리적 메모리에서는 서로 연속적으로 만들었다.  
또한, 블록 단위로 메모리를 배정하기 때문에 한 번에 512개 또는 2048개와 같이 큰 메모리를 예약하지 않고 최대 블록 크기만큼의 메모리만 배정한다.   
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_13.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_13.png)    

위 그림은 블록 크기(페이지 크기)가 4일 때를 가정했다. 입력 토큰은 논리적 KV 블록에서 순서대로 채워지고 생성하는 토큰도 이어서 채운다.  
하지만 실제 GPU 메모리에 저장될 때는 블록 테이블에서 논리적 블록과 물리적 블록이 어떻게 연결되는지 관리한다.  
이처럼 블록 크기가 4이기 때문에 토큰을 생성하면서도 새로운 블록이 필요해질 때도 물리적 메모리를 4칸만 예약한다.  
기존에는 최대 길이 토큰만큼 예약 했을 때와 비교하면 확실하게 메모리를 절약할 수 있다.  
#####  
또한 페이지어텐션은 다양한 디코딩 방식에서 메모리를 절약할 수 있는데, 동일한 입력 프롬프트에서 여러 개의 출력을 생성하는 병렬 샘플링에서   
프롬프트에 대한 메모리를 공유함으로써 메모리를 절약한다.  
병렬 샘플링은 책을 보고 직접 이해하자 P.279 이거를 어떻게 설명해야 될지 감이 안잡힌다.  
하나의 입력 토큰으로 여러개의 결과를 가질때 사용하고 공유 되는 토큰은 참조 카운트를 사용해서 새로운 블록에 복사해서 옮긴다는 개념 자체는 알겠는데  
이게 왜 GPU 메모리에 효율적인지는 잘 모르겠다.  
####

### 추측 디코딩  
쉬운 단어는 더 작고 효율적인 모델이 예측하고 어려운 단어는 더 크고 성능이 좋은 모델이 예측하는 방식을 추측 디코딩이라고 한다. 그럼 그건 어떻게 판단할까??  
추측 디코딩은 작은 드래프트 모델과 큰 타깃 모델이 있다.  
작은 드래프트 모델은 큰 모델에 비해 추론이 빠르므로 K개의 토큰을 먼저 생성한다. 그리고 타깃 모델이 드래프트 모델이 생성한 K개의 토큰이 타깃 모델이 추론했다면  
생성했을 결과와 동일한지 계산해 동일하다면 승인하고 동일하지 않다면 거절한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_14.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/8_14.png)    

위 그림은 K가 2인 경우의 추측 디코딩 과정을 나타냈다.  
먼저 입력 프롬프트+드래프트 모델 생성 2개를 이용해 타깃 모델의 토큰 1개를 생성한다. 이때 타깃 모델은 토큰을 생성하는 과정에서 드래프트 모델이 만든 2개의 토큰을  
타깃 모델이 추론했어도 동일한 토큰을 만들었을지 판단한다.  
이때 동일한 토큰 수를 판단해 그림과 같이 경우의 수를 나누어 문장을 완성해 간다.  
이렇게 되면 타깃 모델 한 번의 추론 시간과 유사한 시간 동안 1~3개의 토큰을 생성할 수 있기 때문에 전체적인 추론 속도가 빨라진다.   
추측 디코딩은 원본 모델에 비해 훨씬 작은 드래프트 모델의 추가만으로 원본 모델의 성능을 유지하면서 속도를 2배이상 높일 수 있다.  
하지만 작은 모델이라도 추가적인 모델이 필요하고 2개의 모델을 사용하기 때문에 시스템 복잡도가 올라간다는 단점이 있다.    

### 실습: LLM 서빙 프레임워크  
실습이니까 코드를 위주로 보자.  
VLLM은 Versatile Large Language Model의 약자로, 대규모 언어 모델의 배포를 더욱 효율적이고 확장 가능하게 만드는 추론 라이브러리이다.  
LLM의 추론은 크게 오프라인 추론과 온라인 추론으로 나눌 수 있다.  
1. 오프라인 추론은 대량의 입력 데이터에 대해 추론을 수행해 충분히 큰 배치 크기를 활용할 수 있는 추론이다.
2. 온라인 추론은 사용자의 요청에 따라 모델 추론을 수행하는 방식을 말한다.

### 오프라인 서빙  
오프라인 서빙이란 정해진 입력 데이터에 대해 배치 추론을 수행하는 것을 말한다.   
모델에 입력할 데이터가 이미 정해져 있기 때문에 배치 처리를 통해 처리량을 높일 수 있다.  
이 뒤에는 전부 코드를 보자.  

### 온라인 서빙  
온라인 서빙은 사용자의 요청이 올 때 모델 추론을 수행한다는 점에서 오프라인 서빙과 다르다.  
여기도 코드를 보자 vllm을 이용해 서빙하는 간단한 코드들이 있다.  

## 정리  
이번 장에서는 LLM의 생성 성능을 떨어뜨리지 않으면서도 모델을 효율적으로 추론할 수 있도록 만드는 다양한 기술들을 살펴봤다.  
1. 입력 데이터마다 생성 길이가 다르기 때문에 일찍 생성이 끝난 데이터는 오랜 시간 대기하고 GPU 사용량은 떨어지는 문제를 해결해 주는 연속 배치 방식
2. GPU의 계측적 구조가 지닌 특성을 활용해 어텐션 연산의 속도를 높은 플래시 어텐션
3. 입력 토큰의 상대적 위치 정보를 전달해 더 긴 입력에서도 생성 성능이 떨어지지 않도록 하는 상대적 위치 인코딩
4. 반복적인 연산에 대해 GPU의 연산 단위인 커널을 결합해 오버헤드를 줄이는 커널 퓨전
5. KV 캐시의 메모리를 효율적으로 관리하는 페이지 어텐션
6. 더 작은 드래프트 모델과 더 성능이 좋은 타깃 모델을 사법 
