# 임베딩 모델로 데이터 의미 압축하기  
텍스트 임베딩 모델이 하는 역할을 더 자세히 알아보기 위해 지금의 임베딩 방식을 사용하기까지 텍스트를 숫자로 표현하려던 다양한 시도를 살펴본다.  
Sentence-Transformers 라이브러리를 통해 문장 임베딩 모델의 구조를 더 자세히 살펴보고 텍스트와 이미지를 임베딩 벡터로 변환하는 실습을 진행한다.  
임베딩 모델은 임베딩 벡터를 사용하기 때문에 문자열이 동일하지 않아도 검색할 수 있다. 임베딩 벡터의 유사도를 기반으로 검색하는 것을 의미 검색이라 부르며 이걸 실습한다.  
의미 검색은 관련도가 떨어지는 문서가 검색될 수 있다는 단점이 있다. 이걸 보완하기 위해 키워드 검ㅈ색과 의미 검색을 조합해 사용하는 하이브리드 검색을 실습한다.  

## 텍스트 임베딩 이해하기  
여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식을 텍스트 임베딩 또는 문장 임베딩이라고 부른다.  
임베딩(embedding)이란 데이터의 의미를 압축한 숫자 배열을 말한다.  
실습 10.1을 보고 오자 학교, 공부, 운동을 예시로 유사도를 이용해 임베딩한 결과를 보여준다.  

### 원핫 인코딩  
위와 같은 예시에서 만약 학교=1 공부=2 운동=3 이라고 가정을 하고 아이디를 붙였다고 생각하자  
그럼 임베딩 자체는 수행했다. 하지만 공부가 학교의 2배 운동은 3배 라는 오해가 생길 수 있다. 이런 문제를 피하기위해 원핫 인코딩이 많이 사용된다.  
원핫 인코딩은 범주형 데이터 사이에 의도하지 않은 관계가 담기는 걸 방지한다는 장점이 있지만, 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 치명적인 단점이 있다.  
실습 10.2를 보고 오면 원핫 인코딩을 할경우 모든 유사도가 0이 나온다.  

### 백오브워즈  
백오브워즈(Bag of Words)는 비슷한 단어가 많이 나오면 비슷한 문장 또는 문서 라는 가정을 활용해 문서를 숫자로 변환한다. 문서는 단어의 빈도를 나타내는 벡터(수치화된 배열)로 변환됩니다.  
만약 경제 기사라면 기본적으로 대출, 증시, 부동산 과 같은 단어를 사용할 가능성이 높다. 물론 it 기사에도 나오겠지만 빈도가 확실히 적을것이다.  
BoW는 아이디어가 직관적이고 구현이 간단함에도 훌륭히 작동하기 때문에 문장과 문서의 의미를 표현하는 방법으로 오랫동안 사용돼 왔다.  
문제는 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는데 크게 도움이 되지 않는 경우가 있다는 점이다.  
예를들어 AI라는 단어는 모든 분야에서 요즘 많이 사용하는 단어라 문서의 의미를 예측하기 어렵다.  

### TF-IDF  
TF-IDF(Term Frequency-Inverse Document Frequency)는 어느 문서에서나 나오는 단어 문제를 보완하기 위해 수식을 활용해 많은 문서에 등장하는 단어의 중요도를 작게 만든다.  
수식에서 TF(w)는 특정 문서에서 특정 단어 w가 등장한 횟수(Term Frequency)이며 DF(w)는 특정 단어 w가 등장한 문서의 수(Document Frequency)이다.  
$$\text{TF-IDF(w)} = \text{TF(w)} \times \log (N/\text{DF(w)})$$  
N이 의미하는 것은 전체 문젓의 수이다.  
책에서 보여준 예제를 보고 이해해보자.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_1.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_1.png)   

조사 "이"는 'LLM'에 비해 IT 기사에서 더 많이 등장하지만 모든 문서에 등장했기 때문에 중요도가 없다.  
하지만 LLM은 0이 아닌 양수이기 때문에 더 중요하다고 인식하게 된다.  
TF-IDF는 BoW의 문제를 성공적으로 보완하면서 오랫동안 활발히 사용됐고, 검색에서도 BM25와 같은 TF-IDF의 변형 방식이 가장 보편적인 연관도 점수 계산 방식으로 사용되고 있다.  
지금까지 살펴본 임베딩은 문서에 등장하는 단어의 수만큼 차원이 커진다. 차원이 커지면 필연적으로 대부분의 수가 0인 벡터가 된다.  
이렇게 대부분이 0인 벡터를 희소하다고 하는데 희소한 벡터는 의미를 압축해서 담고 있지 못하기 때문에 벡터와 벡터 사이의 관계를 활용하기 어렵다.  

뒤에 설명할 워드투벡과 문장 임베딩은 보통 100~1000차원 정도로 압축된 형태인데, 희소한 벡터와 대비해 밀집 임베딩이라고 부른다.  

### 워드투벡  
단어가 함께 등장하는 빈도 정보를 활용해 단어의 의미를 압축하는 단어 임베딩 방법이다.  
특정 단어 주변에 어떤 단어가 있는지 예측하는 모델을 만든다면 단어의 의미를 표현한 임베딩을 모델이 생성할 수 있지 않을까? 라는 가정에 기반해 CBOW 와 스킵 그램 방식이 연구됐다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_2.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_2.png)   
그림을 보면 CBOW 는 주변의 단어 정보로 중간에 있을 단어를 예측하는 방식이다 t번째 단어를 예측하기 위해 위아래로 2개의 단어 정보를 활용한다.  
스킵그램은 반대로 가운데 단어 정보로 주변의 단어를 예측하는데 t번째 단어 정보로 주변 4개의 단어가 무엇인지 맞추는 방식으로 학습한다.  
이렇게 주변 단어를 예측하는 방식을 사용해 학습한 모델로 단어를 임베딩 벡터로 변환하면  
남자와 여자라는 단어의 거리 차이와 왕과 여왕의 거리 차이와 비슷하게 나온다.  
단어의 의미를 압축해 숫자로 표현하니 단어와 단어 사이의 관계를 계산할 수 있고, 그 관계에도 의미가 담겨 있음을 확인한 것이다.  


## 문장 임베딩 방식  
우리가 텍스트를 활용할 때는 단어 단위보다는 문장이나 문단 같은 더 큰 단위를 사용한다. 따라서 여러 단어가 합쳐진 문장을 임베딩 벡터로 변환하는 방법이 필요했다.  
이번 절에서는 문장 사이의 유사도를 계산하는 두 가지 방식을 비교하면서 문장 임베딩 방식의 장점을 살펴본다.  
### 문장 사이의 관계를 계산하는 두 가지 방법  
워드 투벡이 잘 되니까 인공 신경망을 통한 텍스트를 밀집 임베딩으로 표현하는 기술이 개발됐다. 문장 임베딩을 활용하면 문장과 문장 사이의 유사도나 관련성을 벡터 연산을 통해 쉽게 계산할 수 있다는 장점이 있다.  
특히 BERT가 아주 뛰어난 성능을 보였다.  
BERT를 사용한 방법은 크게 두 가지로 나눌 수 있다.  
1. 바이 인코더: 각각의 문장을 독립적으로 BERT에 넣고 모델의 출력결과인 문장 임베딩 벡터를 코사인 유사도로 계산한다.
2. 교차 인코더: 두 문장을 함께 BERT에 넣고 모델이 직접 두 문장 사이의 관계를 0과 1사이의 수로 출력한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_3.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_3.png)  

교차 인코더의 장점은 두 텍스트 사이의 관계를 모두 계산하기 때문에 두 텍스트의 유사도를 정확히 계산할 수 있다는 장점이 있다.  
하지만 입력 문장마다 계산하기 때문에 계산량이 굉장히 많다. 이러한 어텐션 연산은 무거운 연산이므로 가급적 적게 수행해야한다.
또한, 모든 문장 조합에 대해 유사도를 계산해야 가장 유사한 문장을 검색할 수 있어 확장성이 떨어진다.  
이런 문제를 극복하기 위해 바이 인코더가 개발됐다.  
바이 인코더는 각 문장의 독립적인 임베딩을 결과로 반환하기 때문에 유사도를 계산하고 싶은 문장이 바뀌더라도 추가적인 BERT 연산이 필요없다.  
만약 1000개의 문장 중 입력 문장과 가장 유사한 문장을 찾는다면 처음에는 BERT에 1000개의 문장을 넣고 문장 임베딩을 생성한다.  
하지만 다음 다른 문장과 유사한 문장을 찾는다면 1000개의 문장 임베딩을 계산할 필요없이 처음에 구한 문장 임베딩을 그대로 사용하면 되므로  
어텐션 연산은 입력으로 들어오는 문장만 임베딩하면된다.  

### 바이 인코더 모델 구조  
BERT 모델은 입력 토큰마다 출력 임베딩을 생성한다. 하지만 임베딩마다 길이가 각각 다르면 문장과 문장 사이의 유사도를 쉽게 계산하기 어렵다.  
따라서 풀링 층을 사용해 문장을 대표하는 1개의 임베딩으로 통합된다.  
풀링 층을 통해 문장의 길이가 달라져도 1개의 고정된 차원의 임베딩이 반환되기 때문에 코사인 유사도와 같은 거리 계산 방식을 활용해 유사도를 쉽게 계산할 수 있다.  
여기부터는 실습 예제 10.3    

## 실습 의미 검색 구현하기  
실습을 통해 sentence-transformers와 faiss 라이브러리를 활용해 의미 검색을 구현한다.  
의미검색이란 단순히 키워드 매칭을 통한 검색이 아니라 밀집 임베딩을 이용해 문장이나 문서의 의미를 고려한 검색을 수행하는 것을 말한다.  
바로 실습 예제 10.8 ㄱㄱ  

## 검색 방식을 조합해 성능 높이기   
키워드 검색은 의미 검색과 달리 동일한 키워드가 많이 포함될수록 유사도를 높게 평가하는 검색 방식을 말한다.  
따라서 동일한 키워드가 등장한 문서를 상위 검색 결과를 반환하므로 관련성이 떨어지는 결과가 나타날 가능성이 적다.  
하지만 동일한 키워드를 사용하지 않으면 의미가 유사하더라도 검색하지 못한다. 의미 검색과 정 반대의 장단점을 가지고 있다.  
따라서 서로의 장단점을 보완하기 위해 두 검색을 방식을 조합하는 하이브리드 검색을 활용할 수 있다. 여기서는 BM25를 다룬다.  

### 키워드 검색 방식: BM25  
BM25는 앞에서 배운 TF-IDF와 유사한 통계 기반 스코어링 방법으로, TF-IDF에 문서의 길이에 대한 가중치를 추가한 알고리즘이다.  
BM25는 간단하고 계산량이 적으면서도 뛰어난 성능을 보여주고 대표적인 검색 엔진인 일래스틱서치의 기본알고리즘으로 사용된다.  
비교적 최근에 와서야 BM25의 성능을 뛰어 넘는 기술이 연구되고 있을 정도로 강력한 알고리즘이다.   
######
BM25의 수식은 다음과 같다.  
$$\sum_{i=1}^{n}\text{IDF}(q_i) \cdot \frac{f(q_i , D) \cdot (k_1 + 1)}{f(q_i , D) + k_1 \cdot (1-b+b\cdot \frac{|D|}{\text{avgdl}})}$$  

식의 왼쪽 부분인 IDF 쪽은 TF-IDF의 문서 빈도 부분으로 특정 단어 q가 전체 문서에서 얼마나 자주 등장했는지를 나타내고  
오른쪽이 단어 빈도에 대응되는 부분으로 특정 문서(D)내에서 특정 단어가 얼마나 많이 등장했는지를 나타낸다.  
avgdl은 전체 문서의 평균 길이를 나타내고 $$k_1$$과 b는 하이퍼파라미터다.  

IDF 부분을 식으로 풀어쓰면 다음과 같다.  
$$\text{IDF}(q_i)=ln(\frac{N-n(q_i)+0.5}{n(q_i)+0.5}+1) $$  
N은 전체 문서의 수이고 $$n(q_i)$$는 쿼리 단어 q가 등장한 문서의 수이다.  
만약 모든 문서에 q가 등장하면 IDF 부분은 최솟값이 되고 적게 등장하면 IDF 부분의 값은 커진다.  
직관적으로 여러 문서에 등장하는 단어라면 덜 중요한 단어이기 때문에 값이 작아지도록 한다.  
#####  
이제 오른쪽 부분을 살펴본다.  
$$f(q_i,D)$$는 특정 문서 D에 토큰 q가 등장하는 횟수이고, |D|는 D 문서의 길이 avgdl은 전체 문서의 평균 길이다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_4.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_4.png)  
해당 식에서 빨간색 사각형 안에 있는 내용만 보자. 만약 D에 해당 문서에서 q의 등장 빈도가 매우 높아진다면 해당 부분은 $$(k_1 +1)$$에 가까워진다.  
책에는 이렇게 나와있는데 그냥 극한으로 생각했다. q가 무한으로 간다고 생각하니까 이해 가능했다.  
따라서 f가 아무리 커져도 사각형 부분은 $$(k_1 +1)$$으로 수렴하고 무한정 커지지 않기 때문에 단어 빈도에 대한 포화 효과를 나타내며 $$k_1$$ 변수로 이걸 조정한다.  
####
이번엔 반대로 사각형이 아닌 부분만 보면 $$\frac{|D|}{\text{avgdl}}$$ 부분은 문서 길이에 대한 가중치이다.  
만약 평균 문서의 길이가 100이라 가정하면 짧은 문서(|D|가 5일 때)가 긴 문서(|D|가 150 일 때) 해당 부분의 크기가 작아지고 결과적으로 전체 식의 값이 커진다.  
따라서 짧은 문서에 토큰 q가 등장한 경우 더 중요도를 높게 판단한다고 이해 가능하다. 변수 b는 문서 길이 효과를 반영하는 비율을 의미한다 b가 클수록 문서 길이 효과를 더 크게 반영한다.  

![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_5.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_5.png)  

### 상호 순위 조합 이해하기  
하이브리드 검색은 통계 기반 점수와 임베딩 유사도 점수를 하나로 합쳐야 한다. 하지만 두 점수의 분포도는 다르기 때문에 이를 일치 시켜야한다.  
상호 순위 조합(RRF)는 각 점수에서의 순위를 활용해 점수를 산출한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_6.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_6.png)  
위 그림을 보면 이해 가능하다. 여기서 K는 조절 가능한 인자이다.  
그리고 만약 한쪽 점수에서 순위에 들지 못했다면 0점으로 계산한다.  

## 실습: 하이브리드 검색 구현하기  
예제 10.14 보기  

# 정리    
자연어 처리 분야에서 텍스트의 의미를 담아 숫자로 변환하기 위해 개발됐떤 다양한 기술을 살펴봤다. 원핫 인코딩, 백오브워즈 TF-IDF, 워드투벡이 있었다.  
문장 임베딩 방식을 살펴봤다. 바이 인코더와 교차 인코더가 있었고 바이 인코더가 좀 더 활용성이 높아 보였다.
