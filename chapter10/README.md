# 임베딩 모델로 데이터 의미 압축하기  
텍스트 임베딩 모델이 하는 역할을 더 자세히 알아보기 위해 지금의 임베딩 방식을 사용하기까지 텍스트를 숫자로 표현하려던 다양한 시도를 살펴본다.  
Sentence-Transformers 라이브러리를 통해 문장 임베딩 모델의 구조를 더 자세히 살펴보고 텍스트와 이미지를 임베딩 벡터로 변환하는 실습을 진행한다.  
임베딩 모델은 임베딩 벡터를 사용하기 때문에 문자열이 동일하지 않아도 검색할 수 있다. 임베딩 벡터의 유사도를 기반으로 검색하는 것을 의미 검색이라 부르며 이걸 실습한다.  
의미 검색은 관련도가 떨어지는 문서가 검색될 수 있다는 단점이 있다. 이걸 보완하기 위해 키워드 검ㅈ색과 의미 검색을 조합해 사용하는 하이브리드 검색을 실습한다.  

## 텍스트 임베딩 이해하기  
여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식을 텍스트 임베딩 또는 문장 임베딩이라고 부른다.  
임베딩(embedding)이란 데이터의 의미를 압축한 숫자 배열을 말한다.  
실습 10.1을 보고 오자 학교, 공부, 운동을 예시로 유사도를 이용해 임베딩한 결과를 보여준다.  

### 원핫 인코딩  
위와 같은 예시에서 만약 학교=1 공부=2 운동=3 이라고 가정을 하고 아이디를 붙였다고 생각하자  
그럼 임베딩 자체는 수행했다. 하지만 공부가 학교의 2배 운동은 3배 라는 오해가 생길 수 있다. 이런 문제를 피하기위해 원핫 인코딩이 많이 사용된다.  
원핫 인코딩은 범주형 데이터 사이에 의도하지 않은 관계가 담기는 걸 방지한다는 장점이 있지만, 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 치명적인 단점이 있다.  
실습 10.2를 보고 오면 원핫 인코딩을 할경우 모든 유사도가 0이 나온다.  

### 백오브워즈  
백오브워즈(Bag of Words)는 비슷한 단어가 많이 나오면 비슷한 문장 또는 문서 라는 가정을 활용해 문서를 숫자로 변환한다. 문서는 단어의 빈도를 나타내는 벡터(수치화된 배열)로 변환됩니다.  
만약 경제 기사라면 기본적으로 대출, 증시, 부동산 과 같은 단어를 사용할 가능성이 높다. 물론 it 기사에도 나오겠지만 빈도가 확실히 적을것이다.  
BoW는 아이디어가 직관적이고 구현이 간단함에도 훌륭히 작동하기 때문에 문장과 문서의 의미를 표현하는 방법으로 오랫동안 사용돼 왔다.  
문제는 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는데 크게 도움이 되지 않는 경우가 있다는 점이다.  
예를들어 AI라는 단어는 모든 분야에서 요즘 많이 사용하는 단어라 문서의 의미를 예측하기 어렵다.  

### TF-IDF  
TF-IDF(Term Frequency-Inverse Document Frequency)는 어느 문서에서나 나오는 단어 문제를 보완하기 위해 수식을 활용해 많은 문서에 등장하는 단어의 중요도를 작게 만든다.  
수식에서 TF(w)는 특정 문서에서 특정 단어 w가 등장한 횟수(Term Frequency)이며 DF(w)는 특정 단어 w가 등장한 문서의 수(Document Frequency)이다.  
$$\text{TF-IDF(w)} = \text{TF(w)} \times \log (N/\text{DF(w)})$$  
N이 의미하는 것은 전체 문젓의 수이다.  
책에서 보여준 예제를 보고 이해해보자.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_1.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_1.png)   

조사 "이"는 'LLM'에 비해 IT 기사에서 더 많이 등장하지만 모든 문서에 등장했기 때문에 중요도가 없다.  
하지만 LLM은 0이 아닌 양수이기 때문에 더 중요하다고 인식하게 된다.  
TF-IDF는 BoW의 문제를 성공적으로 보완하면서 오랫동안 활발히 사용됐고, 검색에서도 BM25와 같은 TF-IDF의 변형 방식이 가장 보편적인 연관도 점수 계산 방식으로 사용되고 있다.  
지금까지 살펴본 임베딩은 문서에 등장하는 단어의 수만큼 차원이 커진다. 차원이 커지면 필연적으로 대부분의 수가 0인 벡터가 된다.  
이렇게 대부분이 0인 벡터를 희소하다고 하는데 희소한 벡터는 의미를 압축해서 담고 있지 못하기 때문에 벡터와 벡터 사이의 관계를 활용하기 어렵다.  

뒤에 설명할 워드투벡과 문장 임베딩은 보통 100~1000차원 정도로 압축된 형태인데, 희소한 벡터와 대비해 밀집 임베딩이라고 부른다.  

### 워드투벡  
단어가 함께 등장하는 빈도 정보를 활용해 단어의 의미를 압축하는 단어 임베딩 방법이다.  
특정 단어 주변에 어떤 단어가 있는지 예측하는 모델을 만든다면 단어의 의미를 표현한 임베딩을 모델이 생성할 수 있지 않을까? 라는 가정에 기반해 CBOW 와 스킵 그램 방식이 연구됐다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_2.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_2.png)   
그림을 보면 CBOW 는 주변의 단어 정보로 중간에 있을 단어를 예측하는 방식이다 t번째 단어를 예측하기 위해 위아래로 2개의 단어 정보를 활용한다.  
스킵그램은 반대로 가운데 단어 정보로 주변의 단어를 예측하는데 t번째 단어 정보로 주변 4개의 단어가 무엇인지 맞추는 방식으로 학습한다.  
이렇게 주변 단어를 예측하는 방식을 사용해 학습한 모델로 단어를 임베딩 벡터로 변환하면  
남자와 여자라는 단어의 거리 차이와 왕과 여왕의 거리 차이와 비슷하게 나온다.  
단어의 의미를 압축해 숫자로 표현하니 단어와 단어 사이의 관계를 계산할 수 있고, 그 관계에도 의미가 담겨 있음을 확인한 것이다.  


## 문장 임베딩 방식  
우리가 텍스트를 활용할 때는 단어 단위보다는 문장이나 문단 같은 더 큰 단위를 사용한다. 따라서 여러 단어가 합쳐진 문장을 임베딩 벡터로 변환하는 방법이 필요했다.  
이번 절에서는 문장 사이의 유사도를 계산하는 두 가지 방식을 비교하면서 문장 임베딩 방식의 장점을 살펴본다.  
### 문장 사이의 관계를 계산하는 두 가지 방법  
워드 투벡이 잘 되니까 인공 신경망을 통한 텍스트를 밀집 임베딩으로 표현하는 기술이 개발됐다. 문장 임베딩을 활용하면 문장과 문장 사이의 유사도나 관련성을 벡터 연산을 통해 쉽게 계산할 수 있다는 장점이 있다.  
특히 BERT가 아주 뛰어난 성능을 보였다.  
BERT를 사용한 방법은 크게 두 가지로 나눌 수 있다.  
1. 바이 인코더: 각각의 문장을 독립적으로 BERT에 넣고 모델의 출력결과인 문장 임베딩 벡터를 코사인 유사도로 계산한다.
2. 교차 인코더: 두 문장을 함께 BERT에 넣고 모델이 직접 두 문장 사이의 관계를 0과 1사이의 수로 출력한다.  
![https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_3.png](https://github.com/KwanWooPark97/LLM-AI-BOOK/blob/main/img/10_3.png)
